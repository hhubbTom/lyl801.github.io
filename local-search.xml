<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>GBDT</title>
    <link href="/hhubbTom/hhubbTom.github.io/2025/04/01/GBDT/"/>
    <url>/hhubbTom/hhubbTom.github.io/2025/04/01/GBDT/</url>
    
    <content type="html"><![CDATA[<h1 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h1><h2 id="GBDT：梯度提升决策树"><a href="#GBDT：梯度提升决策树" class="headerlink" title="GBDT：梯度提升决策树"></a><strong>GBDT：梯度提升决策树</strong></h2><p><strong>通过构造一组弱的学习器（树），并把多颗决策树的结果累加起来作为最终的预测输出</strong>。该算法将决策树与集成思想进行了有效的结合。</p><p><img src="image.png" alt="image.png"></p><h3 id="（1）boosting的思想"><a href="#（1）boosting的思想" class="headerlink" title="（1）boosting的思想"></a>（1）boosting的思想</h3><p>Boosting方法训练基分类器时采用串行的方式，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。</p><p>![image 1](image 1.png)</p><h3 id="（2）GBDT原理"><a href="#（2）GBDT原理" class="headerlink" title="（2）GBDT原理"></a>（2）GBDT原理</h3><ul><li>所有弱分类器的结果相加等于预测值。</li><li>每次都以当前预测为基准，下一个弱分类器去拟合误差函数（上一次的误差值）对预测值的残差（预测值与真实值之间的误差）。</li><li>GBDT的弱分类器使用的是树模型（第一张图）。</li></ul><p>![image.png](image 2.png)</p><p>用GBDT去预测年龄：</p><p>第一个弱分类器（第一棵树）预测一个年龄（如20岁），计算发现误差有10岁；<br>第二棵树预测拟合残差，预测值6，计算发现差距还有4岁；<br>第三棵树继续预测拟合残差，预测值3，发现差距只有1岁了；<br>第四课树用1岁拟合剩下的残差，完成。<br>最终，四棵树的结论加起来，得到30岁这个标注答案（实际工程实现里，GBDT是计算负梯度，用负梯度近似残差）。</p><p>![image.png](image 3.png)</p><h3 id="（3）GBDT训练过程"><a href="#（3）GBDT训练过程" class="headerlink" title="（3）GBDT训练过程"></a>（3）GBDT训练过程</h3><p>假定训练集只有4个人 (A,B,C,D)，他们的年龄分别是 (14,16,24,26)。其中，A、B分别是高一和高三学生；C、D分别是应届毕业生和工作两年的员工。</p><p>我们先看看用回归树来训练，得到的结果如下图所示：</p><p>![image.png](image 4.png)</p><p>接下来改用GBDT来训练。由于样本数据少，我们限定叶子节点最多为2（即每棵树都只有一个分枝），并且限定树的棵树为2。最终训练得到的结果如下图所示：</p><p>![image.png](image 5.png)</p><p>上图中的树很好理解：A、B年龄较为相近，C、D年龄较为相近，被分为左右两支，每支用平均年龄作为预测值。</p><p>我们计算残差（即「实际值」-「预测值」），所以A的残差14-15&#x3D;-1。</p><p>这里A的「预测值」是指前面所有树预测结果累加的和，在当前情形下前序只有一棵树，所以直接是15，其他多树的复杂场景下需要累加计算作为A的预测值。</p><p>![image.png](image 6.png)</p><p>上图中的树就是残差学习的过程了</p><p>把A、B、C、D的值换作残差-1、1、-1、1，再构建一棵树学习，这棵树只有两个值1和-1，直接分成两个节点：A、C在左边，B、D在右边。</p><p>这棵树学习残差，在我们当前这个简单的场景下，已经能保证预测值和实际值（上一轮残差）相等了。</p><p>我们把这棵树的预测值累加到第一棵树上的预测结果上，就能得到真实年龄，这个简单例子中每个人都完美匹配，得到了真实的预测值。</p><p>![image.png](image 7.png)</p><p>最终的预测过程是这样的：</p><p>A：高一学生，购物较少，经常问学长问题，真实年龄14岁，预测年龄A &#x3D; 15 – 1 &#x3D; 14<br>B：高三学生，购物较少，经常被学弟提问，真实年龄16岁，预测年龄B &#x3D; 15 + 1 &#x3D; 16<br>C：应届毕业生，购物较多，经常问学长问题，真实年龄24岁，预测年龄C &#x3D; 25 – 1 &#x3D; 24<br>D：工作两年员工，购物较多，经常被学弟提问，真实年龄26岁，预测年龄D &#x3D; 25 + 1 &#x3D; 26<br>综上，GBDT需要将多棵树的得分累加得到最终的预测得分，且每轮迭代，都是在现有树的基础上，增加一棵新的树去拟合前面树的预测值与真实值之间的残差。</p><h3 id="梯度提升-VS-梯度下降"><a href="#梯度提升-VS-梯度下降" class="headerlink" title="梯度提升 VS 梯度下降"></a>梯度提升 VS 梯度下降</h3><p>梯度提升：是在函数空间里更新</p><p>梯度下降：是在参数空间里更新</p><p>![image.png](image 8.png)</p><h3 id="GBDT的优缺点"><a href="#GBDT的优缺点" class="headerlink" title="GBDT的优缺点"></a>GBDT的优缺点</h3><p>![image.png](image 9.png)</p><h2 id="GBDT方法：LightGBM"><a href="#GBDT方法：LightGBM" class="headerlink" title="GBDT方法：LightGBM"></a>GBDT方法：LightGBM</h2><p>GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间</p><p>LightGBM的设计初衷就是提供一个快速高效、低内存占用、高准确度、支持并行和大规模数据处理的数据科学工具</p><p>重要特点：优化准确率，使用leaf-wise生长方式，可以处理分类变量</p><h3 id="（1）XGBoost的缺点"><a href="#（1）XGBoost的缺点" class="headerlink" title="（1）XGBoost的缺点"></a>（1）XGBoost的缺点</h3><p>XGBoost的基本思想</p><ul><li>首先，对所有特征都按照特征的数值进行<strong>预排序</strong>。</li><li>其次，在遍历分割点的时候用O(#data)的代价找到一个特征上的最好分割点。</li><li>最后，在找到一个特征的最好分割点后，将数据分裂成左右子节点。</li></ul><p><strong>总结XGBoost：预排序；Level-wise的层级生长策略；特征对梯度的访问是一种随机访问。</strong></p><p>这样的<strong>预排序算法</strong>的优点是能精确地找到分割点。但是缺点也很明显：</p><p>首先，<strong>空间消耗大</strong>。这样的算法需要保存数据的特征值信息，还保存了特征排序的结果（例如，为了后续快速的计算分割点，保存了排序后的索引），这就需要消耗训练数据两倍的内存。</p><p>其次，<strong>时间上也有较大的开销</strong>，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。</p><p>最后，<strong>对cache优化不友好</strong>。在预排序后，特征对梯度的访问是一种随机访问，<strong>并且不同的特征访问的顺序不一样</strong>，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。</p><h3 id="（2）LightGBM"><a href="#（2）LightGBM" class="headerlink" title="（2）LightGBM"></a>（2）LightGBM</h3><p><strong>LightGBM：基于Histogram的决策树算法；Leaf-wise的叶子生长策略；Cache命中率优化；直接支持类别特征（categorical Feature）</strong></p><p>LightGBM有哪些实现，各有什么区别？<br>答：gbdt:梯度提升决策树，串行速度慢，容易过拟合；rf：随机森林，并行速度快；dart：训练较慢；goss：容易过拟合。</p><p>LightGBM原理：和GBDT及XGBoost类似，都采用损失函数的负梯度作为当前决策树的残差近似值，去拟合新的决策树。</p><p>LightGBM树的生长方式是<strong>垂直方向</strong>的，其他的算法都是水平方向的，也就是说LightGBM生长的是树的叶子，其他的算法生长的是树的层次。<br>LightGBM选择具有<strong>最大误差的树叶进行生长（更改权重思想）</strong>，当生长同样的树叶，生长叶子的算法可以比基于层的算法减少更多的loss。</p><p>不建议在小数据集上使用LightGBM。LightGBM对过拟合很敏感，对于小数据集非常容易过拟合。对于过拟合的解决方法：Leaf-wise之上增加了一个最大深度的限制。</p><p>通俗解释：LGB的优化方法是，在保留大梯度（残差大）样本的同时，随机地保留一些小梯度样本，同时放大了小梯度样本带来的信息增益。</p><p>这样说起来比较抽象，我们过一遍流程： 首先把样本按照梯度排序，选出梯度最大的a%个样本，然后在剩下小梯度数据中随机选取b%个样本，在计算信息增益的时候，将选出来b%个小梯度样本的信息增益扩大（ 1 - a） &#x2F; b 倍。这样就会避免对于数据分布的改变。</p><h3 id="基于Histogram的决策树算法"><a href="#基于Histogram的决策树算法" class="headerlink" title="基于Histogram的决策树算法"></a><strong>基于Histogram的决策树算法</strong></h3><p>直方图算法的基本思想是：先把连续的浮点特征值离散化成K个整数，同时构造一个宽度为K的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。</p><p>直方图算法优点：内存占用更小，计算代价更小。</p><p>Histogram算法找到的分割点并不是很精确，但对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；较粗的分割点也有正则化的效果，可以有效地防止过拟合。</p><p>差加速：LightGBM另一个优化是Histogram（直方图）做差加速。一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到，在速度上可以提升一倍。</p><p>![image.png](image 10.png)</p><h3 id="单边梯度采样算法"><a href="#单边梯度采样算法" class="headerlink" title="单边梯度采样算法"></a>单边梯度采样算法</h3><p>GOSS算法从减少样本的角度出发，排除大部分小梯度的样本，仅用剩下的样本计算信息增益，即保留大梯度数据和部分小梯度数据，部分小梯度数据乘以一个系数，使得训练不足的样本得到更多关注</p><h2 id="LightGBM的优化"><a href="#LightGBM的优化" class="headerlink" title="LightGBM的优化"></a>LightGBM的优化</h2><p>（1） ****直接支持类别特征</p><p>（2）支持高效并行</p><p>（3）Cache命中率优化</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>BN、LN</title>
    <link href="/hhubbTom/hhubbTom.github.io/2025/04/01/BN%E3%80%81LN/"/>
    <url>/hhubbTom/hhubbTom.github.io/2025/04/01/BN%E3%80%81LN/</url>
    
    <content type="html"><![CDATA[<h1 id="BN、LN"><a href="#BN、LN" class="headerlink" title="BN、LN"></a>BN、LN</h1><p><strong>一、什么是Normalization？</strong></p><p>Normalization：规范化或标准化，就是把输入数据X，在输送给神经元之前先对其进行平移和伸缩变换，将X的分布规范化成在<strong>固定区间范围</strong>的标准分布。</p><p><img src="image.png" alt="image"></p><p><strong>二、深度学习中为什么要用Normalization？</strong></p><p>把数据拉回标准<strong>正态分布</strong>，因为神经网络的Block大部分都是矩阵运算，一个向量经过矩阵运算后值会越来越大，产生各种各样的分布，梯度会不稳定。**为了网络的稳定性，加快网络的收敛，防止过拟合，且便于比较，**我们需要及时把值拉回正态分布。</p><p>Normalization根据标准化操作的维度不同可以分为batch Normalization和Layer Normalization。BatchNorm就是通过对batch size这个维度归一化来让分布稳定下来。LayerNorm则是通过对Hidden size这个维度归一化来让某层的分布稳定。</p><p><strong>三、Batch Normalization 和 Layer Normalization的定义</strong></p><p>BN（纵向规范化）：</p><p> ![image 1](image 1.png)</p><p>LN（横向规范化）：</p><p>![image 2](image 2.png)</p><p><strong>四、Batch Normalization 和 Layer Normalization的区别</strong></p><p>BatchNorm是对一个batch-size样本内的每个特征做归一化，**抹杀了同一样本内不同特征之间的大小关系，但是保留了不同样本间的大小关系；**LayerNorm是对每个样本的所有特征一起做归一化，<strong>抹杀了不同样本间的大小关系，但是保留了一个样本内不同特征之间的大小关系</strong>。</p><p><strong>（理解：<strong>BN对batch</strong>不同数据的同一特征</strong>进行标准化，变换之后，纵向来看，不同样本的同一特征仍然保留了之前的大小关系，但是横向对比样本内部的各个特征之间的大小关系不一定和变换之前一样了，因此抹杀或破坏了不同特征之间的大小关系，保留了不同样本之间的大小关系；LN对<strong>单一样本的所有特征</strong>进行标准化，样本内的特征处理后原来数值大的还是相对较大，原来数值小的还是相对较小，不同特征之间的大小关系还是保留了下来，但是不同样本在各自标准化处理之后，两个样本对应位置的特征之间的大小关系将不再确定，可能和处理之前就不一样了，所以破坏了不同样本间的大小关系**）**</p><p><strong>五、Batch Normalization 和 Layer Normalization的使用场景</strong></p><p>在BN和LN都能使用的场景中，BN的效果一般优于LN，原因是基于不同数据，同一特征得到的归一化特征<strong>更不容易损失信息</strong>。但是有些场景是不能使用BN的，例如batch size较小或者序列问题中使用LN。</p><p><strong>RNN 或Transformer为什么用Layer Normalization？</strong></p><p><strong>首先</strong>RNN或Transformer解决的是序列问题，一个存在的问题是不同样本的序列长度不一致，而Batch Normalization需要对不同样本的同一位置特征进行标准化处理，所以无法应用；当然，输入的序列都要做padding补齐操作，但是补齐的位置填充的都是0，这些位置都是无意义的，此时的标准化也就没有意义了。</p><p><strong>其次</strong>上面说到，BN抹杀了不同特征之间的大小关系；LN是保留了一个样本内不同特征之间的大小关系，这对NLP任务是至关重要的。对于NLP或者序列任务来说，一条样本的不同特征，其实就是时序上的变化，这正是需要学习的东西，自然不能做归一化抹杀，所以要用LN。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/hhubbTom/hhubbTom.github.io/2025/03/22/hello-world/"/>
    <url>/hhubbTom/hhubbTom.github.io/2025/03/22/hello-world/</url>
    
    <content type="html"><![CDATA[<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
