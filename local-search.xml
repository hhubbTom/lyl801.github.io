<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>手撕梯度下降、LR、交叉熵、Softmax</title>
    <link href="/hhubbTom/hhubbTom.github.io/2025/04/01/%E6%89%8B%E6%92%95%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E3%80%81LR%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E3%80%81Softmax/"/>
    <url>/hhubbTom/hhubbTom.github.io/2025/04/01/%E6%89%8B%E6%92%95%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E3%80%81LR%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E3%80%81Softmax/</url>
    
    <content type="html"><![CDATA[<p>部分深度学习基本理论的手撕代码，一些内容有借鉴CSDN和知乎相关帖子和笔记</p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_loss</span>(<span class="hljs-params">w, b, x, y</span>):<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">计算二次损失函数</span><br><span class="hljs-string"></span><br><span class="hljs-string">参数：</span><br><span class="hljs-string">w (float): 权重</span><br><span class="hljs-string">b (float): 偏置</span><br><span class="hljs-string">x (float): 输入值</span><br><span class="hljs-string">y (float): 真实标签</span><br><span class="hljs-string"></span><br><span class="hljs-string">返回：</span><br><span class="hljs-string">float: 损失值</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>y_pred = w * x + b<br>loss = <span class="hljs-number">0.5</span> * (y - y_pred) ** <span class="hljs-number">2</span>  <span class="hljs-comment"># 二次损失</span><br><span class="hljs-keyword">return</span> loss<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_gradients</span>(<span class="hljs-params">w, b, x, y</span>):<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">计算损失函数对w和b的梯度</span><br><span class="hljs-string">参数：</span><br><span class="hljs-string">w (float): 权重</span><br><span class="hljs-string">b (float): 偏置</span><br><span class="hljs-string">x (float): 输入值</span><br><span class="hljs-string">y (float): 真实标签</span><br><span class="hljs-string"></span><br><span class="hljs-string">返回：</span><br><span class="hljs-string">tuple: 返回梯度（dw, db）</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>y_pred = w * x + b<br>dw = -(y - y_pred) * x  <span class="hljs-comment"># 对w的梯度</span><br>db = -(y - y_pred)      <span class="hljs-comment"># 对b的梯度</span><br><span class="hljs-keyword">return</span> dw, db<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient_descent</span>(<span class="hljs-params">x, y, learning_rate=<span class="hljs-number">0.1</span>, num_iterations=<span class="hljs-number">100</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    基于梯度下降法优化w和b</span><br><span class="hljs-string">    参数：</span><br><span class="hljs-string">    x (array): 输入值数组</span><br><span class="hljs-string">    y (array): 真实标签数组</span><br><span class="hljs-string">    w_init (float): 初始权重</span><br><span class="hljs-string">    b_init (float): 初始偏置</span><br><span class="hljs-string">    learning_rate (float): 学习率</span><br><span class="hljs-string">    num_iterations (int): 迭代次数</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回：</span><br><span class="hljs-string">    tuple: 最终的优化参数w和b</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    w = <span class="hljs-number">0</span><br>    b = <span class="hljs-number">0</span><br>    loss_history = []<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iterations):<br>        dw, db = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>        total_loss = <span class="hljs-number">0</span>  <span class="hljs-comment"># 用于累加所有样本的损失值</span><br>        <span class="hljs-comment"># 计算所有样本的梯度</span><br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x)):<br>            dw_i, db_i = compute_gradients(w, b, x[j], y[j])<br>            dw += dw_i<br>            db += db_i<br>            total_loss += compute_loss(w, b, x[j], y[j])  <span class="hljs-comment"># 累加损失值</span><br><br>        <span class="hljs-comment"># 求平均梯度</span><br>        dw /= <span class="hljs-built_in">len</span>(x)<br>        db /= <span class="hljs-built_in">len</span>(x)<br><br>        <span class="hljs-comment"># 更新权重和偏置</span><br>        w -= learning_rate * dw<br>        b -= learning_rate * db<br><br>        <span class="hljs-comment"># 计算平均损失并记录</span><br>        average_loss = total_loss / <span class="hljs-built_in">len</span>(x)  <span class="hljs-comment"># 计算平均损失</span><br>        loss_history.append(average_loss)<br><br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Iteration <span class="hljs-subst">&#123;i&#125;</span>, Loss: <span class="hljs-subst">&#123;average_loss:<span class="hljs-number">.4</span>f&#125;</span>, w: <span class="hljs-subst">&#123;w:<span class="hljs-number">.4</span>f&#125;</span>, b: <span class="hljs-subst">&#123;b:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><br>    <span class="hljs-keyword">return</span> w, b, loss_history<br><br><span class="hljs-comment"># 示例数据（简单的线性回归问题）</span><br><br>x_data = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>])<br>y_data = np.array([<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>, <span class="hljs-number">11</span>])  <span class="hljs-comment"># y = 2 * x + 1</span><br><br><span class="hljs-comment"># 使用梯度下降优化w和b</span><br><br>w_final, b_final, loss_history = gradient_descent(x_data, y_data, learning_rate=<span class="hljs-number">0.1</span>, num_iterations=<span class="hljs-number">100</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;最终的权重: w = <span class="hljs-subst">&#123;w_final:<span class="hljs-number">.4</span>f&#125;</span>, 偏置: b = <span class="hljs-subst">&#123;b_final:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="LR"><a href="#LR" class="headerlink" title="LR"></a>LR</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># Sigmoid函数</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">z</span>):<br><span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-z))<br><br><span class="hljs-comment"># 损失函数：交叉熵</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_loss</span>(<span class="hljs-params">y, y_hat</span>):<br>m = <span class="hljs-built_in">len</span>(y)<br>loss = -np.<span class="hljs-built_in">sum</span>(y * np.log(y_hat) + (<span class="hljs-number">1</span> - y) * np.log(<span class="hljs-number">1</span> - y_hat)) / m<br><span class="hljs-keyword">return</span> loss<br><br><span class="hljs-comment"># 梯度下降实现</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">logistic_regression</span>(<span class="hljs-params">X, y, learning_rate=<span class="hljs-number">0.01</span>, num_iterations=<span class="hljs-number">1000</span></span>):<br>m, n = X.shape  <span class="hljs-comment"># m是样本数，n是特征数</span><br>w = np.zeros(n)<br>b = <span class="hljs-number">0</span><br>loss_history = []<br><br><span class="hljs-comment"># 梯度下降</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iterations):<br>    <span class="hljs-comment"># 计算线性模型输出</span><br>    z = np.dot(X, w) + b<br>    y_hat = sigmoid(z)<br><br>    <span class="hljs-comment"># 计算损失</span><br>    loss = compute_loss(y, y_hat)<br>    loss_history.append(loss)<br><br>    <span class="hljs-comment"># 计算梯度</span><br>    dw = np.dot(X.T, (y_hat - y)) / m<br>    db = np.<span class="hljs-built_in">sum</span>(y_hat - y) / m<br><br>    <span class="hljs-comment"># 更新参数</span><br>    w -= learning_rate * dw<br>    b -= learning_rate * db<br><br>    <span class="hljs-comment"># 每100次迭代输出一次损失</span><br>    <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Iteration <span class="hljs-subst">&#123;i&#125;</span>, Loss: <span class="hljs-subst">&#123;loss:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><br><span class="hljs-keyword">return</span> w, b, loss_history<br><br><span class="hljs-comment"># 示例数据（简单二分类问题）</span><br><br>X = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>], [<span class="hljs-number">6</span>, <span class="hljs-number">7</span>]])  <span class="hljs-comment"># 输入特征</span><br>y = np.array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])  <span class="hljs-comment"># 真实标签</span><br><br><span class="hljs-comment"># 训练逻辑回归模型</span><br><br>w, b, loss_history = logistic_regression(X, y, learning_rate=<span class="hljs-number">0.1</span>, num_iterations=<span class="hljs-number">1000</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练完成的权重:&quot;</span>, w)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练完成的偏置:&quot;</span>, b)<br></code></pre></td></tr></table></figure><h3 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 二分类交叉熵</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">binary_cross_entropy</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算二分类交叉熵损失</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    参数：</span><br><span class="hljs-string">    y_true (numpy array): 真实标签，0 或 1</span><br><span class="hljs-string">    y_pred (numpy array): 模型预测的概率（预测为正类的概率）</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    返回：</span><br><span class="hljs-string">    float: 计算得到的交叉熵损失</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 防止log(0)，对预测概率做裁剪</span><br>    epsilon = <span class="hljs-number">1e-15</span><br>    y_pred = np.clip(y_pred, epsilon, <span class="hljs-number">1.</span> - epsilon)<br>    <br>    <span class="hljs-comment"># 交叉熵公式： - [y * log(p) + (1 - y) * log(1 - p)]</span><br>    loss = - (y_true * np.log(y_pred) + (<span class="hljs-number">1</span> - y_true) * np.log(<span class="hljs-number">1</span> - y_pred))<br>    <span class="hljs-keyword">return</span> np.mean(loss)<br><br><span class="hljs-comment"># 示例：真实标签和预测概率</span><br>y_true = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>])<br>y_pred = np.array([<span class="hljs-number">0.9</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.2</span>])<br><br><span class="hljs-comment"># 计算交叉熵损失</span><br>loss = binary_cross_entropy(y_true, y_pred)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Binary Cross-Entropy Loss: <span class="hljs-subst">&#123;loss&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 多分类交叉熵</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">categorical_cross_entropy</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算多分类交叉熵损失</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    参数：</span><br><span class="hljs-string">    y_true (numpy array): 真实标签的one-hot编码（例如 [0, 1, 0]）</span><br><span class="hljs-string">    y_pred (numpy array): 模型预测的类别概率分布</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    返回：</span><br><span class="hljs-string">    float: 计算得到的交叉熵损失</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 防止log(0)，对预测概率做裁剪</span><br>    epsilon = <span class="hljs-number">1e-15</span><br>    y_pred = np.clip(y_pred, epsilon, <span class="hljs-number">1.</span> - epsilon)<br>    <br>    <span class="hljs-comment"># 交叉熵公式： - sum(p * log(q))</span><br>    loss = - np.<span class="hljs-built_in">sum</span>(y_true * np.log(y_pred), axis=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> np.mean(loss)<br><br><span class="hljs-comment"># 示例：真实标签和预测概率（one-hot编码）</span><br>y_true = np.array([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])<br>y_pred = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.1</span>], [<span class="hljs-number">0.9</span>, <span class="hljs-number">0.05</span>, <span class="hljs-number">0.05</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.8</span>], [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.1</span>], [<span class="hljs-number">0.8</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>]])<br><br><span class="hljs-comment"># 计算交叉熵损失</span><br>loss = categorical_cross_entropy(y_true, y_pred)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Categorical Cross-Entropy Loss: <span class="hljs-subst">&#123;loss&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">z</span>):<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">计算Softmax函数</span><br><span class="hljs-string">参数：</span><br><span class="hljs-string">z (numpy array): 输入向量</span><br><span class="hljs-string">返回：</span><br><span class="hljs-string">numpy array: Softmax后的概率分布</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>exp_z = np.exp(z - np.<span class="hljs-built_in">max</span>(z))  <span class="hljs-comment"># 防止溢出，减去最大值</span><br><span class="hljs-keyword">return</span> exp_z / np.<span class="hljs-built_in">sum</span>(exp_z)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_derivative</span>(<span class="hljs-params">z</span>):<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">计算Softmax的导数</span><br><span class="hljs-string">参数：</span><br><span class="hljs-string">z (numpy array): 输入向量</span><br><span class="hljs-string"></span><br><span class="hljs-string">返回：</span><br><span class="hljs-string">numpy array: Softmax导数矩阵</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>p = softmax(z)<br>S = np.diag(p) - np.outer(p, p)  <span class="hljs-comment"># 雅可比矩阵</span><br><span class="hljs-keyword">return</span> S<br><br><span class="hljs-comment"># 示例</span><br><br>z = np.array([<span class="hljs-number">2.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.1</span>])  <span class="hljs-comment"># 模型的输出（未归一化的得分）</span><br>softmax_output = softmax(z)<br>softmax_derivative_output = softmax_derivative(z)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Softmax Output:&quot;</span>, softmax_output)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Softmax Derivative Matrix:\n&quot;</span>, softmax_derivative_output)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/hhubbTom/hhubbTom.github.io/2025/03/22/hello-world/"/>
    <url>/hhubbTom/hhubbTom.github.io/2025/03/22/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
