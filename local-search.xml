<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Attention-Transformer</title>
    <link href="/hhubbTom/hhubbTom.github.io/2025/04/04/Attention-Transformer/"/>
    <url>/hhubbTom/hhubbTom.github.io/2025/04/04/Attention-Transformer/</url>
    
    <content type="html"><![CDATA[<h1 id="Attention-Transformer"><a href="#Attention-Transformer" class="headerlink" title="Attention-Transformer"></a>Attention-Transformer</h1><h2 id="基础理论"><a href="#基础理论" class="headerlink" title="基础理论"></a>基础理论</h2><h3 id="attention机制"><a href="#attention机制" class="headerlink" title="attention机制"></a><strong>attention机制</strong></h3><p><img src="image.png" alt="image.png"></p><p>三要素：查询、键、值，查询和键生成权重（值的重要性），与对应值相乘，再汇总得到输出。</p><p>三要素的现实含义：查询相当于（自己的）需求，键相当于需求对应的（对方的）条件，查询乘键就是反映需求和条件的契合度，值就是最终显式化展现出的情况，即应用查询和键的契合度（权重）。</p><p>缩放点积：要求查询和键长度相同。</p><p><img src="image%201.png" alt="image.png"></p><h3 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a><strong>多头注意力</strong></h3><p>当给定相同的查询、键和值的集合时， 我们希望模型可以<strong>基于相同的注意力机制学习到不同的行为， 然后将不同的行为作为知识组合起来</strong>， 捕获序列内各种范围的依赖关系（例如，短距离依赖和长距离依赖关系）。 因此，允许注意力机制组合使用查询、键和值的不同<em>子空间表示</em>（representation subspaces）可能是有益的。</p><p>为此，与其只使用单独一个注意力汇聚， 我们可以用独立学习得到的h组不同的 <em>线性投影</em>（linear projections）来变换查询、键和值。 然后，这h组变换后的查询、键和值将并行地送到注意力汇聚中。 最后，将这h个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出，这种设计被称为<em>多头注意力</em>（multihead attention）。</p><p><img src="image%202.png" alt="image.png"></p><p>处理公式：</p><p><img src="image%203.png" alt="image.png"></p><p>最终输出：</p><p><img src="image%204.png" alt="image.png"></p><h3 id="自注意力"><a href="#自注意力" class="headerlink" title="自注意力"></a>自注意力</h3><p>查询、键、值来自同一组词元（同样的输入），实际是经过了矩阵线性变换得到的，自注意力的作用是建立输入序列之间的长距离依赖关系。</p><p><strong>卷积神经网络、循环神经网络、自注意力机制对比</strong></p><p><img src="image%205.png" alt="image.png"></p><p>卷积神经网络和自注意力都拥有并行计算的优势， 而且自注意力的最大路径长度最短。 但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。</p><h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a><strong>位置编码</strong></h3><p>在处理词元序列时，循环神经网络是逐个的重复地处理词元的， 而自注意力则因为并行计算而放弃了顺序操作。 为了使用序列的顺序信息，通过在输入表示中添加 <em>位置编码</em>（positional encoding）来注入绝对的或相对的位置信息。</p><p>位置编码方式：基于正弦函数和余弦函数的固定位置编码，绝对位置信息编码（二进制），相对位置信息编码。</p><h3 id="Transformer（自注意力机制）"><a href="#Transformer（自注意力机制）" class="headerlink" title="Transformer（自注意力机制）"></a><strong>Transformer（自注意力机制）</strong></h3><p><img src="image%206.png" alt="image.png"></p><p>Transformer的编码器和解码器是基于自注意力的模块叠加而成的，源（输入）序列和目标（输出）序列的<em>嵌入</em>（embedding）表示将加上<em>位置编码</em>（positional encoding），再分别输入到编码器和解码器中。</p><p>从宏观角度来看，Transformer的编码器是由多个相同的层叠加而成的，每个层都有两个子层（子层表示为sublayer）。第一个子层是<em>多头自注意力</em>（multi-head self-attention）汇聚；第二个子层是<em>基于位置的前馈网络</em>（positionwise feed-forward network），它由全连接层和非线性模块组成。</p><p>具体来说，在计算编码器的自注意力时，查询、键和值都来自前一个编码器层的输出。每个子层都采用了<em>残差连接</em>（residual connection），在残差连接的加法计算之后，紧接着应用<em>层规范化</em>（layer normalization）。<br>Transformer解码器也是由多个相同的层叠加而成的，并且层中使用了残差连接和层规范化。除了编码器中描述的两个子层之外，解码器还在这两个子层之间插入了第三个子层，称为<em>编码器－解码器注意力</em>（encoder-decoder attention）层。在编码器－解码器注意力中，查询来自前一个解码器层的输出，而键和值来自整个编码器的输出。</p><p>在解码器自注意力中，查询、键和值都来自上一个解码器层的输出。但是，解码器中的每个位置只能考虑该位置之前的所有位置。这种<em>掩蔽</em>（masked）注意力保留了<em>自回归</em>（auto-regressive）属性，确保预测仅依赖于已生成的输出词元。</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a><strong>问题</strong></h2><h2 id="1-Transformer-网络结构"><a href="#1-Transformer-网络结构" class="headerlink" title="1. Transformer 网络结构"></a><strong>1. Transformer 网络结构</strong></h2><h3 id="1-1-介绍-Transformer-的整体结构"><a href="#1-1-介绍-Transformer-的整体结构" class="headerlink" title="1.1 介绍 Transformer 的整体结构"></a><strong>1.1 介绍 Transformer 的整体结构</strong></h3><p>Transformer 是一种基于注意力机制（Attention）的深度学习模型结构，其特点是不依赖于循环（RNN）或卷积（CNN）。整体架构分为 <strong>Encoder-Decoder</strong> 两部分：</p><p><strong>Encoder</strong>：由多个相同的层（layer）堆叠而成，每一层包括两部分：多头注意力机制（Multi-head Self-Attention）和前馈神经网络（Feed Forward Neural Network）。</p><p><strong>Decoder</strong>：与 Encoder 类似，但每层多了一个对 Encoder 输出的注意力模块（Encoder-Decoder Attention）。</p><p>Encoder和Decoder包含残差连接和层规范化。</p><h3 id="1-2-介绍-Transformer-中-Encoder-和-Decoder-的结构"><a href="#1-2-介绍-Transformer-中-Encoder-和-Decoder-的结构" class="headerlink" title="1.2 介绍 Transformer 中 Encoder 和 Decoder 的结构"></a><strong>1.2 介绍 Transformer 中 Encoder 和 Decoder 的结构</strong></h3><p><strong>Encoder</strong> 结构：</p><p>（1）输入的序列通过嵌入层（Embedding）和位置编码（Positional Encoding）。</p><p>（2）使用多头自注意力（Multi-head Self-Attention）捕获序列中的相关性。</p><p>（3）后接前馈神经网络（FFN）。</p><p>（4）每层后有残差连接和 Layer Normalization。</p><p><strong>Decoder</strong> 结构：</p><p>（1）输入（目标序列）先通过嵌入层和位置编码。</p><p>（2）第一个模块是 Masked Multi-head Attention，确保目标序列的每个位置只依赖于其前面的位置。</p><p>（3）第二个模块是 Encoder-Decoder Attention，捕获 Encoder 的输出与目标序列之间的关系。</p><p>（4）后接 FFN，再通过全连接层输出结果。</p><h3 id="1-3-介绍-Encoder-和-Decoder-的区别，为什么这样设计？"><a href="#1-3-介绍-Encoder-和-Decoder-的区别，为什么这样设计？" class="headerlink" title="1.3 介绍 Encoder 和 Decoder 的区别，为什么这样设计？"></a><strong>1.3 介绍 Encoder 和 Decoder 的区别，为什么这样设计？</strong></h3><p><strong>区别</strong>：</p><p>（1）Encoder 处理的是输入序列。</p><p>（2）Decoder 处理的是目标序列，并从 Encoder 获取信息。</p><p><strong>设计原因</strong>：</p><p>（1）Encoder 负责提取输入序列的全局表示。</p><p>（2）Decoder 生成输出时需要同时关注目标序列自身的上下文以及输入序列的上下文。</p><h3 id="1-4-介绍-Transformer-的输入输出流程"><a href="#1-4-介绍-Transformer-的输入输出流程" class="headerlink" title="1.4 介绍 Transformer 的输入输出流程"></a><strong>1.4 介绍 Transformer 的输入输出流程</strong></h3><p>（1）输入序列经过嵌入和位置编码。</p><p>（2）输入通过 Encoder 层逐步提取特征，得到上下文表示。</p><p>（3）Decoder 接收目标序列（带偏移的输入）和 Encoder 的输出，逐步生成目标序列。</p><h3 id="1-5-Encoder-和-Decoder-的数据输入有什么区别？"><a href="#1-5-Encoder-和-Decoder-的数据输入有什么区别？" class="headerlink" title="1.5 Encoder 和 Decoder 的数据输入有什么区别？"></a><strong>1.5 Encoder 和 Decoder 的数据输入有什么区别？</strong></h3><p><strong>Encoder 输入</strong>：原始输入序列经过嵌入和位置编码。</p><p><strong>Decoder 输入</strong>：目标序列经过嵌入和位置编码，同时引入 Mask 来隐藏未来的信息。</p><h3 id="1-6-Encoder-与-Decoder-之间如何进行数据传输？"><a href="#1-6-Encoder-与-Decoder-之间如何进行数据传输？" class="headerlink" title="1.6 Encoder 与 Decoder 之间如何进行数据传输？"></a><strong>1.6 Encoder 与 Decoder 之间如何进行数据传输？</strong></h3><p>Decoder的Masked多头注意力模块输出作为查询，Encoder 模块的输出作为 Decoder 的键和值，经过 Encoder-Decoder Attention 模块，帮助 Decoder 关注输入序列中与当前目标位置相关的信息。</p><h3 id="1-7-介绍残差连接及其作用"><a href="#1-7-介绍残差连接及其作用" class="headerlink" title="1.7 介绍残差连接及其作用"></a><strong>1.7 介绍残差连接及其作用</strong></h3><p>残差连接是一种结构优化方法，在Transformer中，残差连接将模块的输入与输出相加后再进行层归一化（LayerNorm）。</p><p>残差连接的作用：</p><p>**（1）防止梯度消失：**在深层网络中，梯度在反向传播过程中可能会逐层衰减，导致梯度消失，影响训练。残差连接允许梯度直接从后面几层传播回前面几层，避免梯度信息在中间层丢失，从而缓解梯度消失问题，帮助深层模型更稳定地训练。</p><p>**（2）保留原始输入信息：**残差连接直接将输入信息传递到当前层的输出，确保模型在优化时不会丢失原始输入特征。这种机制可以让模型在学习复杂特征时，不会完全忽略输入的低级特征，从而提高模型的表达能力。</p><p>**（3）加速训练收敛：**残差连接使得每一层学习的是一个 <strong>残差函数</strong>（即偏离恒等映射的部分），而不是直接学习复杂的非线性变换。通过这种方式，每一层只需要学习相对于输入的小变化，降低了学习难度，加速了模型的收敛。</p><p>**（4）防止退化问题：**在深度网络中，随着网络层数的增加，模型性能可能会出现退化现象（即增加网络层数反而导致性能下降）。残差连接通过直接传递输入，避免了深层网络中的退化问题，使得 Transformer 可以堆叠更多层，提高模型的表达能力。</p><p>**（5）便于特征融合：**残差连接将每一层的输入特征与经过多头注意力机制或前馈网络处理后的输出特征相加。这种特征融合方式，可以帮助模型有效组合不同层次的特征表示，捕捉更丰富的语义信息。</p><h3 id="1-8-Transformer使用什么激活函数？为什么？"><a href="#1-8-Transformer使用什么激活函数？为什么？" class="headerlink" title="1.8 Transformer使用什么激活函数？为什么？"></a><strong>1.8 Transformer使用什么激活函数？为什么？</strong></h3><p>采用GELU激活函数，它是一种平滑的，具有连续性导数的激活函数，形式上近似高斯分布，有助于减少梯队消失和提高网络标识能力，加快收敛速度。GELU的公式为x·P(X≤x)，其中x是均值为0，方差为1的高斯随机变量。</p><h2 id="2-Attention"><a href="#2-Attention" class="headerlink" title="2. Attention"></a><strong>2. Attention</strong></h2><h3 id="2-1-介绍-Attention-机制和公式"><a href="#2-1-介绍-Attention-机制和公式" class="headerlink" title="2.1 介绍 Attention 机制和公式"></a><strong>2.1 介绍 Attention 机制和公式</strong></h3><p>Attention 的核心思想是计算序列中每个位置的重要性分数，公式为：</p><p><img src="image%207.png" alt="image.png"></p><p>Q: 查询向量（Query）</p><p>K: 键向量（Key）</p><p>V: 值向量（Value）</p><p>$sqrt(d_k)$:  缩放因子，dk代表键向量的维度，用于防止梯度消失。</p><h3 id="2-2-Attention-中的可学习参数是什么？"><a href="#2-2-Attention-中的可学习参数是什么？" class="headerlink" title="2.2 Attention 中的可学习参数是什么？"></a><strong>2.2 Attention 中的可学习参数是什么？</strong></h3><p>Q、K、V 的线性变换矩阵是可学习参数。</p><h3 id="2-3-Q⋅K-的数学意义是什么？"><a href="#2-3-Q⋅K-的数学意义是什么？" class="headerlink" title="2.3 Q⋅K 的数学意义是什么？"></a><strong>2.3 Q⋅K 的数学意义是什么？</strong></h3><p>Q⋅K表示查询与键之间的相似性，即注意力分数。</p><h3 id="2-4-Attention-中为什么使用缩放点积-scale？"><a href="#2-4-Attention-中为什么使用缩放点积-scale？" class="headerlink" title="2.4 Attention 中为什么使用缩放点积 scale？"></a><strong>2.4 Attention 中为什么使用缩放点积 scale？</strong></h3><p>（1）维持数值稳定：点积的结果会随着向量维度dk增大而增大，用缩放因子可以防止内积值过大造成的数值不稳定。</p><p>（2）保证梯度稳定：避免 Softmax 函数输入过大进入梯度消失区域，scale可以让输入值分布更平缓，使得 Softmax 的梯度更稳定，从而提高模型训练的效率和稳定性。</p><p>（3）减少维度带来的偏差：Attention 机制的核心思想是通过点积计算查询 Q 和键 K 的相关性。但当 dk 较大时，点积结果的值域会变宽，可能导致某些特定位置的权重被过度放大，而其他位置的权重被忽略。缩放因子能够对不同维度的影响进行归一化，减少因向量维度导致的偏差。</p><h3 id="2-5-Attention-中为什么使用-softmax？"><a href="#2-5-Attention-中为什么使用-softmax？" class="headerlink" title="2.5 Attention 中为什么使用 softmax？"></a><strong>2.5 Attention 中为什么使用 softmax？</strong></h3><p>Softmax 将分数转换为概率分布，用于加权值向量。</p><h3 id="2-6-Q-和-K-能否用同一个投影矩阵？"><a href="#2-6-Q-和-K-能否用同一个投影矩阵？" class="headerlink" title="2.6 Q 和 K 能否用同一个投影矩阵？"></a><strong>2.6 Q 和 K 能否用同一个投影矩阵？</strong></h3><p>理论上可以，但这样会减少模型表达能力，降低泛化性和模型效果。Q和K的含义不同，需要不同的矩阵去学习不同的语义特征和表示，来捕捉它们之间更复杂的依赖关系。如果用同一个矩阵，模型很可能会失去区分能力。</p><h3 id="2-7-为什么-Attention-可以堆叠多层，有什么作用？"><a href="#2-7-为什么-Attention-可以堆叠多层，有什么作用？" class="headerlink" title="2.7 为什么 Attention 可以堆叠多层，有什么作用？"></a><strong>2.7 为什么 Attention 可以堆叠多层，有什么作用？</strong></h3><p>多层 Attention允许每一层逐步<strong>提取和融合更高阶的特征表示</strong>，进而提高模型的表达能力和性能，<strong>捕获更深的关系</strong>，提升模型泛化能力。其中，残差连接+层规范化为Attention堆叠多层提供了保证。</p><h3 id="2-8-Decoder-中的-Mask-Attention-如何实现-mask？mask有哪些作用？"><a href="#2-8-Decoder-中的-Mask-Attention-如何实现-mask？mask有哪些作用？" class="headerlink" title="2.8 Decoder 中的 Mask Attention 如何实现 mask？mask有哪些作用？"></a><strong>2.8 Decoder 中的 Mask Attention 如何实现 mask？mask有哪些作用？</strong></h3><p>在计算Attention分数时，将被填充为0的Mask位置给一个负无穷的数，使得Softmax输出为零。</p><p>作用：使当前token的生成只依赖过去生成的token，保持自回归属性，防止未来信息泄露；在序列填充补齐时mask，使被填充的位置权重为0，避免无关信息参与attention的运算，使计算更高效；保证attention的局部性，token的交融是有限的。</p><h3 id="2-9-手写Attention代码？"><a href="#2-9-手写Attention代码？" class="headerlink" title="2.9 手写Attention代码？"></a><strong>2.9 手写Attention代码？</strong></h3><p>def attention(query, key, value, mask&#x3D;None, dropout&#x3D;None)</p><p>d_k &#x3D; query.size(-1)</p><p>scores &#x3D; torch.matmul(query, key.transpose(-2, -1))&#x2F;sqrt(d_k)</p><p>if mask is not None:</p><p>scores &#x3D; scores.masked_fill(mask&#x3D;&#x3D;0, -1e9)</p><p>att_weight &#x3D; torch.softmax(scores, dim &#x3D; -1)</p><p>if dropout is not None:</p><p>att_weight &#x3D; dropout(att_weight)</p><p>att_value &#x3D; torch.matmul(att_weight, value)</p><p>return att_weight, att_value</p><h3 id="2-10-Attention计算自注意力机制用softmax函数有什么不合理之处？"><a href="#2-10-Attention计算自注意力机制用softmax函数有什么不合理之处？" class="headerlink" title="2.10 Attention计算自注意力机制用softmax函数有什么不合理之处？"></a>2.10 Attention计算自注意力机制用softmax函数有什么不合理之处？</h3><p>**（1）对大数值的敏感性：**Softmax对输入的数值范围非常敏感。当注意力得分（如点积得分）数值非常大或非常小的时候，softmax可能会导致梯度消失或梯度爆炸，影响模型的稳定和训练效率。</p><p><strong>（2）计算复杂度</strong>：在计算自注意力时，对于每个输入的向量都需要计算一对一的注意力得分并应用softmax，这会导致计算复杂度较高，尤其是当序列长度很大时。Softmax操作需要对每个序列的所有位置计算指数和归一化，复杂度为O(N^2)，其中N是序列的长度。</p><p>**（3）精度损失：**由于softmax在数值较大或较小时的溢出或下溢问题，可能会导致计算精度的损失。尤其是在处理长序列时，这种数值不稳定性可能会影响模型的性能。</p><p><strong>（4）硬性归一化</strong>：Softmax强制要求所有注意力权重和为1，这使得模型可能会过于依赖某些特定的输入部分，即使这些部分实际上并不重要。对于一些任务，可能不需要对所有注意力权重进行严格归一化，某些方法（如稀疏注意力）尝试引入更多灵活性。</p><p><strong>（5）难以捕捉长距离依赖</strong>：尽管自注意力机制可以捕捉任意两个位置之间的依赖关系，但softmax的归一化方式可能使得远距离的依赖关系相对较弱，因为它们的注意力得分通常会较小，而被近距离的得分压制。因此，有时模型在长序列中的表现不如在较短序列中好。</p><h3 id="2-11-计算Attention-为什么是用点乘而不是加法？"><a href="#2-11-计算Attention-为什么是用点乘而不是加法？" class="headerlink" title="2.11 计算Attention 为什么是用点乘而不是加法？"></a><strong>2.11 计算Attention 为什么是用点乘而不是加法？</strong></h3><p>（1）点乘作为注意力权重计算时效率更高，虽然矩阵加法计算上更简单，但计算注意力权重时需要考虑所有可能的键和查询的匹配情况，计算量仍然很大，而点乘可以通过矩阵乘法高效实现这一过程。在大规模数据和复杂模型下，点乘具有比加法更好的性能。</p><p>（2）表达能力更强，点乘更能表示查询和键的相似度&#x2F;相关性，如果使用加法，仅会产生一个新的向量，但没有量化两个向量之间的相似度。</p><p>（3）点乘更能让模型捕捉到向量空间中的结构性信息，衡量每个位置对其它位置的依赖程度，在训练时更加高效。而加法模型必须引入额外的非线性变换捕捉这样的关系，但是会增加计算复杂度。</p><h3 id="2-12-简单谈下Attention的各个变体有哪些？"><a href="#2-12-简单谈下Attention的各个变体有哪些？" class="headerlink" title="2.12 简单谈下Attention的各个变体有哪些？"></a>2.12 简单谈下Attention的各个变体有哪些？</h3><p>MHA：多头注意力机制，下面有介绍。</p><p>Sparse Attention：仅计算输入序列中部分元素间注意力分数，在推荐系统稀疏特征中常用。</p><p>Linear Attention：去掉softmax，先算K转置V，复杂度更加接近线性。</p><p>MQA：用于解决KV缓存的显存压力，缓解让所有Q共享一个KV，缺点是MQA对KV缓存压缩太严重，影响效率。</p><h2 id="3-Multi-head-Attention"><a href="#3-Multi-head-Attention" class="headerlink" title="3. Multi-head Attention"></a><strong>3. Multi-head Attention</strong></h2><h3 id="3-1-如何实现-Multi-head-Attention？"><a href="#3-1-如何实现-Multi-head-Attention？" class="headerlink" title="3.1 如何实现 Multi-head Attention？"></a><strong>3.1 如何实现 Multi-head Attention？</strong></h3><ol><li>将输入分别通过多个独立的线性变换得到多组 Q、K、V。</li><li>在不同子空间下，分别对每组 Q、K、V 计算 Attention。</li><li>将所有头的输出拼接后，再通过线性变换得到最终结果。</li></ol><h3 id="3-2-采用-Multi-head-的好处是什么？"><a href="#3-2-采用-Multi-head-的好处是什么？" class="headerlink" title="3.2 采用 Multi-head 的好处是什么？"></a><strong>3.2 采用 Multi-head 的好处是什么？</strong></h3><p>不同头可以关注输入序列中的<strong>不同特征表示</strong>，能获得序列<strong>不同的关注点</strong>，能学习到<strong>不同的行为</strong>， 然后作为知识组合起来， 捕获序列内各种范围的依赖关系，增强模型的表达能力。</p><h3 id="3-3-Multi-head-Attention-的物理意义是什么？"><a href="#3-3-Multi-head-Attention-的物理意义是什么？" class="headerlink" title="3.3 Multi-head Attention 的物理意义是什么？"></a><strong>3.3 Multi-head Attention 的物理意义是什么？</strong></h3><p>模拟人类对复杂问题的多角度关注。</p><h3 id="3-4-采用-Multi-head-是否增加了计算的时间复杂度？"><a href="#3-4-采用-Multi-head-是否增加了计算的时间复杂度？" class="headerlink" title="3.4 采用 Multi-head 是否增加了计算的时间复杂度？"></a><strong>3.4 采用 Multi-head 是否增加了计算的时间复杂度？</strong></h3><p>是的，但通过并行计算可减少实际耗时。</p><h3 id="3-5-Q、K、V矩阵维度与-head-数的关系？为什么要对每个头进行降维？"><a href="#3-5-Q、K、V矩阵维度与-head-数的关系？为什么要对每个头进行降维？" class="headerlink" title="3.5 Q、K、V矩阵维度与 head 数的关系？为什么要对每个头进行降维？"></a><strong>3.5 Q、K、V矩阵维度与 head 数的关系？为什么要对每个头进行降维？</strong></h3><p>假设输入维度为 dmodel，head 数为 h，每个 head 的维度为 dmodel&#x2F;h。</p><p>通过降维的方式，减少矩阵参数量，处理起来计算量更小，计算量分散到每个头上，能更加保持计算高效性（实验证明），并且输出维度和输入维度保持一致。</p><h3 id="3-6-Multi-head-与-Single-head-之间如何转换？"><a href="#3-6-Multi-head-与-Single-head-之间如何转换？" class="headerlink" title="3.6 Multi-head 与 Single-head 之间如何转换？"></a><strong>3.6 Multi-head 与 Single-head 之间如何转换？</strong></h3><p>Multi-head 是 Single-head 的扩展，可以将多个 head 的输入合并为单一头来实现转换。</p><h2 id="4-位置编码"><a href="#4-位置编码" class="headerlink" title="4. 位置编码"></a><strong>4. 位置编码</strong></h2><h3 id="4-1-Transformer-中如何使输入序列具有相对位置关系？"><a href="#4-1-Transformer-中如何使输入序列具有相对位置关系？" class="headerlink" title="4.1 Transformer 中如何使输入序列具有相对位置关系？"></a><strong>4.1 Transformer 中如何使输入序列具有相对位置关系？</strong></h3><p>Transformer 中没有像 RNN 那样的循环结构，输入序列的顺序信息需要通过位置编码（Positional Encoding）来显式地添加到每个输入向量中，从而使模型能够捕获序列的相对和绝对位置关系（特别是相同单词在不同位置时的情况）。</p><p>位置编码为每个序列位置生成一个唯一的向量，这个向量与输入的嵌入向量（Embedding）相加，作为每个位置的输入。</p><p>不同位置的编码具有规律性，能够体现相对位置信息，比如通过三角函数或其他编码方式，保证模型可以推断两个位置之间的相对距离。</p><h3 id="4-2-Transformer-中的位置编码如何实现？"><a href="#4-2-Transformer-中的位置编码如何实现？" class="headerlink" title="4.2 Transformer 中的位置编码如何实现？"></a><strong>4.2 Transformer 中的位置编码如何实现？</strong></h3><p>Transformer 原始论文使用 <strong>固定的正余弦函数</strong> 进行位置编码，公式如下：</p><p><img src="image%208.png" alt="image.png"></p><p>pos 表示位置，i 表示当前向量的维度索引，dmodel​ 是向量的维度大小。</p><p>位置编码是一个固定的矩阵，不需要训练。</p><p>编码向量维度与嵌入向量一致（dmodel​），方便与嵌入向量相加。</p><p><strong>实现步骤：</strong></p><ol><li>对每个位置生成一组正弦和余弦值。</li><li>偶数维度使用正弦函数，奇数维度使用余弦函数。</li><li>编码结果与输入嵌入相加后送入 Transformer 网络。</li></ol><h3 id="4-3-为什么使用三角函数作为位置编码函数？"><a href="#4-3-为什么使用三角函数作为位置编码函数？" class="headerlink" title="4.3 为什么使用三角函数作为位置编码函数？"></a><strong>4.3 为什么使用三角函数作为位置编码函数？</strong></h3><ol><li><strong>捕获相对位置信息：</strong><ul><li>三角函数的周期性和连续性使得不同位置之间的距离信息通过编码自然体现。</li><li>例如，两个位置的编码之间的点积会随位置差的变化有一定规律性，模型可以通过简单操作学习相对位置关系。</li></ul></li><li><strong>无需训练：</strong><ul><li>正余弦函数生成的位置编码是固定的，不依赖训练，减少了模型参数。</li></ul></li><li><strong>通用性强：</strong><ul><li>三角函数的数学性质（如加法公式）使得编码具有一定的对称性和可泛化性，能很好地嵌入输入序列的位置信息。</li></ul></li></ol><h3 id="4-4-Transformer-中原始的位置编码有何优缺点？"><a href="#4-4-Transformer-中原始的位置编码有何优缺点？" class="headerlink" title="4.4 Transformer 中原始的位置编码有何优缺点？"></a><strong>4.4 Transformer 中原始的位置编码有何优缺点？</strong></h3><p><strong>优点：</strong></p><ol><li><strong>无需训练参数：</strong> 使用固定函数生成位置编码，节省了模型的训练资源。</li><li><strong>捕获相对位置关系：</strong> 三角函数的规律性能够隐式表示序列中的相对位置。</li><li><strong>简单易用：</strong> 正余弦编码与嵌入向量直接相加，无需额外操作。</li></ol><p><strong>缺点：</strong></p><ol><li><strong>固定编码：</strong> 对所有任务都使用相同的固定编码，可能限制模型对某些任务的优化能力。</li><li><strong>不灵活：</strong> 不能动态调整，无法直接适应变长序列或局部特征更重要的场景。</li><li><strong>对长序列性能不足：</strong> 当序列长度非常大时，高维位置编码可能不足以捕捉远距离依赖。</li></ol><h3 id="4-5-介绍-Transformer-中常用的位置编码方法及优缺点"><a href="#4-5-介绍-Transformer-中常用的位置编码方法及优缺点" class="headerlink" title="4.5 介绍 Transformer 中常用的位置编码方法及优缺点"></a><strong>4.5 介绍 Transformer 中常用的位置编码方法及优缺点</strong></h3><p>除了原始的正余弦位置编码，Transformer 中还使用了多种位置编码方法，包括 <strong>可训练位置编码</strong> 和 <strong>相对位置编码</strong>。</p><ol><li><strong>固定位置编码（正余弦编码）：</strong><ul><li><strong>描述</strong>：使用正余弦函数生成位置编码，位置信息固定。</li><li><strong>优点</strong>：简单高效，无需额外训练。</li><li><strong>缺点</strong>：缺乏灵活性，不能动态调整。</li></ul></li><li><strong>可训练的位置编码：</strong><ul><li><strong>描述</strong>：位置编码不再固定，而是像嵌入向量一样作为可训练参数。</li><li><strong>优点</strong>：更加灵活，能够针对不同任务学习最优的位置信息表示。</li><li><strong>缺点</strong>：增加了模型参数，训练时可能引入过拟合风险。</li></ul></li><li><strong>相对位置编码（Relative Position Encoding）：</strong><ul><li><strong>描述</strong>：不直接表示位置，而是编码序列中位置间的相对距离。<ul><li>如 Transformer-XL 和 T5 使用相对位置编码。</li></ul></li><li><strong>优点</strong>：<ol><li>更自然地捕捉序列中各位置间的关系，适合处理长序列。</li><li>对序列长度的变化更加鲁棒。</li></ol></li><li><strong>缺点</strong>：实现复杂性较高，可能增加计算开销。</li></ul></li><li><strong>复合位置编码：</strong><ul><li><strong>描述</strong>：结合固定位置编码和可训练位置编码的优点，例如 DeBERTa 引入的混合方法。</li><li><strong>优点</strong>：平衡了灵活性和泛化能力。</li><li><strong>缺点</strong>：需要更复杂的模型设计。</li></ul></li><li><strong>旋转位置编码（RoPE, Rotary Position Embedding）：</strong><ul><li><strong>描述</strong>：通过旋转变换将位置信息嵌入 Attention 计算中。</li><li><strong>优点</strong>：<ol><li>能够自然地捕捉相对位置信息。</li><li>适用于多种模型（GPT、BERT）。</li></ol></li><li><strong>缺点</strong>：实现难度较高，计算复杂度略微增加。</li></ul></li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><ul><li><strong>正余弦位置编码</strong> 适合简单任务，无需额外参数。</li><li><strong>可训练位置编码</strong> 提高了灵活性，适合复杂场景。</li><li><strong>相对位置编码</strong> 能更好捕捉序列间相对关系，适合长序列任务。</li><li>选择位置编码方法时需权衡计算复杂度、参数量和模型性能之间的关系。</li></ul><h2 id="5-NLP-Transformer-vs-RNN"><a href="#5-NLP-Transformer-vs-RNN" class="headerlink" title="5. NLP: Transformer vs RNN"></a>5. NLP: Transformer vs RNN</h2><h3 id="5-1-Transformer-相比于-RNN-的优势是什么？"><a href="#5-1-Transformer-相比于-RNN-的优势是什么？" class="headerlink" title="5.1 Transformer 相比于 RNN 的优势是什么？"></a><strong>5.1 Transformer 相比于 RNN 的优势是什么？</strong></h3><ol><li><p><strong>高效的并行计算：</strong></p><p> （1）RNN 由于其序列依赖性，无法并行处理输入序列，处理长序列时效率低。</p><p> （2）Transformer 使用自注意力机制（Self-Attention）代替序列依赖，所有输入可以同时处理，多头的多组attention也能同时计算，大大提升了计算效率。</p></li><li><p><strong>捕捉长距离依赖：</strong></p><p> （1）RNN 只能逐步传播信息，导致长序列中的远距离依赖难以捕捉（如梯度消失）。</p><p> （2）Transformer 的注意力机制可以直接在任意两个位置间计算权重，天然适合处理长距离依赖。</p></li><li><p><strong>性能更优：</strong></p><p> Transformer 模型可以在更大的数据集上训练，参数更丰富，效果优于 RNN。</p></li><li><p><strong>更容易扩展：</strong></p><p> RNN 和 LSTM 在处理非常长的序列时效果下降，而 Transformer 可以很好地扩展到超长序列（如 GPT-4 处理 8k 或更长的上下文）。</p></li></ol><h3 id="5-2-Transformer-的并行计算体现在哪里？推理时是并行还是串行的？"><a href="#5-2-Transformer-的并行计算体现在哪里？推理时是并行还是串行的？" class="headerlink" title="5.2 Transformer 的并行计算体现在哪里？推理时是并行还是串行的？"></a><strong>5.2 Transformer 的并行计算体现在哪里？推理时是并行还是串行的？</strong></h3><ol><li><p><strong>训练时：</strong></p><p> <strong>训练是并行的：</strong></p><p> （1）自注意力机制（Self-Attention）中，每个位置的向量可以同时计算所有其他位置的相关性（权重矩阵），即并行处理整个输入序列。</p><p> （2）前馈网络中的运算（如矩阵乘法）也可以并行执行。</p><p> （3）位置编码为固定的，不存在额外的依赖。</p></li><li><p><strong>推理时：</strong></p><p> <strong>推理是串行的：</strong></p><p> （1）在生成任务（如文本生成）中，Transformer 是自回归的（Autoregressive），当前时刻的输出依赖于之前的输出，因此推理是逐步生成的。</p><p> （2）对于非生成任务（如分类或翻译），可以一次性并行计算整个序列。</p></li></ol><h3 id="5-3-Transformer-为什么效果好？RNN-为什么效果不好？"><a href="#5-3-Transformer-为什么效果好？RNN-为什么效果不好？" class="headerlink" title="5.3 Transformer 为什么效果好？RNN 为什么效果不好？"></a><strong>5.3 Transformer 为什么效果好？RNN 为什么效果不好？</strong></h3><ol><li><p><strong>Transformer 效果好的原因：</strong></p><p> <strong>（1）自注意力机制：</strong> 直接建模全局依赖关系，避免了 RNN 的信息递归传播问题。</p><p> <strong>（2）并行计算：</strong> 提高了效率，同时可以处理更大的数据。</p><p> <strong>（3）强大的表达能力：</strong> 多层堆叠和多头注意力增强了特征提取能力。</p><p> <strong>（4）规模化训练：</strong> Transformer 更容易扩展到超大规模模型（如 GPT-3&#x2F;4 和 BERT）。</p></li><li><p><strong>RNN 效果不好的原因：</strong></p><p> <strong>（1）梯度问题：</strong> 由于序列依赖，RNN 容易出现梯度消失或梯度爆炸，限制了学习能力。</p><p> <strong>（2）长距离依赖：</strong> 随着序列长度增加，远距离信息难以传播。</p><p> <strong>（3）效率低下：</strong> RNN 的序列化计算无法并行，限制了计算速度。</p></li></ol><h3 id="5-4-NLP-任务中使用-RNN-和-Transformer-的区别是什么？"><a href="#5-4-NLP-任务中使用-RNN-和-Transformer-的区别是什么？" class="headerlink" title="5.4 NLP 任务中使用 RNN 和 Transformer 的区别是什么？"></a><strong>5.4 NLP 任务中使用 RNN 和 Transformer 的区别是什么？</strong></h3><p><img src="image%209.png" alt="image.png"></p><h3 id="5-5-RNN-为什么不需要使用位置编码？"><a href="#5-5-RNN-为什么不需要使用位置编码？" class="headerlink" title="5.5 RNN 为什么不需要使用位置编码？"></a><strong>5.5 RNN 为什么不需要使用位置编码？</strong></h3><p>**RNN 自带序列信息：**RNN 的递归结构天然保留了输入序列的顺序信息，每一步的隐藏状态 ht 是基于前一时刻 ht−1 和当前输入 xt 的，序列顺序隐式包含在计算过程中，时间维度本身就反映了输入序列的位置关系，因此不需要额外位置编码。</p><h3 id="5-6-介绍-BERT-的网络结构？"><a href="#5-6-介绍-BERT-的网络结构？" class="headerlink" title="5.6 介绍 BERT 的网络结构？"></a><strong>5.6 介绍 BERT 的网络结构？</strong></h3><p><strong>BERT（Bidirectional Encoder Representations from Transformers）</strong> 是一种基于 Transformer 的双向编码模型，专注于自然语言理解任务。</p><ol><li><p><strong>总体结构：</strong></p><p> （1）BERT 使用了 <strong>Transformer 编码器（Encoder）部分</strong>，由多层自注意力机制和前馈神经网络堆叠组成。</p><p> （2）双向建模：通过 Masked Language Model（MLM），BERT 能够同时从左到右和从右到左学习上下文信息。</p></li><li><p><strong>关键组成部分：</strong></p><p> <strong>（1）输入表示：</strong></p><p> 输入由三部分组成：Token Embedding、Segment Embedding、Position Embedding，且输入序列长度固定（如 512）。</p><p> <strong>（2）自注意力机制：</strong></p><p> 多头注意力捕捉序列中全局信息。</p><p> <strong>（3）前馈网络：</strong></p><p> 每层后接一个全连接层进行特征映射。</p><p> <strong>（4）堆叠层数：</strong></p><p> BERT-Base 有 12 层 Transformer 编码器，BERT-Large 有 24 层。</p></li><li><p><strong>训练目标：</strong></p><p> <strong>（1）Masked Language Model（MLM）：</strong> 随机掩盖部分词，预测被掩盖的词。</p><p> <strong>（2）Next Sentence Prediction（NSP）：</strong> 判断两个句子是否连续。</p></li><li><p><strong>特点：</strong></p><p> <strong>（1）双向上下文：</strong> 同时从左右两个方向建模。</p><p> <strong>（2）通用性强：</strong> 预训练后可以通过微调应用于多种 NLP 任务。</p></li></ol><h2 id="6-transformer的KV缓存机制"><a href="#6-transformer的KV缓存机制" class="headerlink" title="6. transformer的KV缓存机制"></a>6. transformer的KV缓存机制</h2><h3 id="6-1-为什么transformer要做KV缓存？"><a href="#6-1-为什么transformer要做KV缓存？" class="headerlink" title="6.1 为什么transformer要做KV缓存？"></a><strong>6.1 为什么transformer要做KV缓存？</strong></h3><p>transformer的token是逐个生成的，每次新的预测会基于之前所有token信息进行矩阵运算，这样会使处理速度随数据顺序逐渐降低，复杂度和耗时度增加。KV缓存就是减少这种耗时的矩阵运算，在transformer推理过程中进行缓存，在生成后续token时直接访问缓存，无需重复计算，提高效率。</p><h3 id="6-2-KV缓存工作机制是怎样的？"><a href="#6-2-KV缓存工作机制是怎样的？" class="headerlink" title="6.2 KV缓存工作机制是怎样的？"></a><strong>6.2 KV缓存工作机制是怎样的？</strong></h3><p>引入KV缓存机制后，我们只需要计算最新token的注意力，先前的token注意力无需重复计算。参考下图：</p><p><img src="554f08a03c8fea992555094d9049e51.jpg" alt="554f08a03c8fea992555094d9049e51.jpg"></p><p><img src="0d2badcfe64167cfa040c6372f76bfc.jpg" alt="0d2badcfe64167cfa040c6372f76bfc.jpg"></p><h3 id="6-3-带有KV缓存优化的大模型推理过程包含几个阶段？"><a href="#6-3-带有KV缓存优化的大模型推理过程包含几个阶段？" class="headerlink" title="6.3 带有KV缓存优化的大模型推理过程包含几个阶段？"></a>6.3 带有KV缓存优化的大模型推理过程包含几个阶段？</h3><p>（1）prefill阶段：输入一个prompt序列，为每个transformer生成KV缓存，同时输出第一个token。</p><p>（2）decoding阶段：发生在计算第二个输出token至最后一个token中，缓存此时的值，每轮推理只需读取缓存，同时将当前轮计算的新K、V追加写入缓存。</p><h3 id="6-4-如何估算KV缓存消耗的显存大小呢？"><a href="#6-4-如何估算KV缓存消耗的显存大小呢？" class="headerlink" title="6.4 如何估算KV缓存消耗的显存大小呢？"></a>6.4 如何估算KV缓存消耗的显存大小呢？</h3><p>KV缓存通常是储存的16位张量。对于一个token，KV缓存会为每一层和每个注意力头储存一对KV张量。总显存消耗公式&#x3D;层数×KV注意力头数量×注意力头的维度×（位宽&#x2F;8）×2，8代表8位一个字节，×2代表K、V两组张量。</p><h3 id="6-5-KV缓存有什么缺陷？如何解决？"><a href="#6-5-KV缓存有什么缺陷？如何解决？" class="headerlink" title="6.5 KV缓存有什么缺陷？如何解决？"></a>6.5 KV缓存有什么缺陷？如何解决？</h3><p>缺陷：当文本长度增加，模型结构变复杂后，KV缓存的显存将大幅增加。</p><p>解决方案：量化KV缓存，例如从16位降低到4位，但是量化可能会减慢解码过程，并且可能会显著影响LLM的准确性。实际需要通过调整量化超参数，重写量化算子，以及融合其他一些算子，来提高解码速度。</p><h2 id="完整多头注意力机制代码"><a href="#完整多头注意力机制代码" class="headerlink" title="完整多头注意力机制代码"></a>完整多头注意力机制代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## 多头注意力机制</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, embed_size, heads, dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-built_in">super</span>(MultiHeadAttention, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.embed_size = embed_size<br>        <span class="hljs-variable language_">self</span>.heads = heads<br>        <span class="hljs-variable language_">self</span>.head_dim = embed_size // heads<br>        <br>        <span class="hljs-keyword">assert</span> <span class="hljs-variable language_">self</span>.head_dim * heads == embed_size, <span class="hljs-string">&quot;Embedding size must be divisible by number of heads&quot;</span><br>        <br>        <span class="hljs-comment"># 定义 Q, K, V 的线性变换</span><br>        <span class="hljs-variable language_">self</span>.query = nn.Linear(embed_size, embed_size)<br>        <span class="hljs-variable language_">self</span>.key = nn.Linear(embed_size, embed_size)<br>        <span class="hljs-variable language_">self</span>.value = nn.Linear(embed_size, embed_size)<br>        <br>        <span class="hljs-comment"># 输出线性变换</span><br>        <span class="hljs-variable language_">self</span>.fc_out = nn.Linear(embed_size, embed_size)<br>        <br>        <span class="hljs-comment"># Dropout层</span><br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(dropout)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, values, keys, query, mask=<span class="hljs-literal">None</span></span>):<br>        N = query.shape[<span class="hljs-number">0</span>]<br>        value_len, key_len, query_len = values.shape[<span class="hljs-number">1</span>], keys.shape[<span class="hljs-number">1</span>], query.shape[<span class="hljs-number">1</span>]<br>        <br>        <span class="hljs-comment"># 1. 通过 Q, K, V 的线性变换得到 Q, K, V</span><br>        Q = <span class="hljs-variable language_">self</span>.query(query)<br>        K = <span class="hljs-variable language_">self</span>.key(keys)<br>        V = <span class="hljs-variable language_">self</span>.value(values)<br>        <br>        <span class="hljs-comment"># 2. 将 Q, K, V 切分成多个头</span><br>        Q = Q.reshape(N, query_len, <span class="hljs-variable language_">self</span>.heads, <span class="hljs-variable language_">self</span>.head_dim).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># (N, heads, query_len, head_dim)</span><br>        K = K.reshape(N, key_len, <span class="hljs-variable language_">self</span>.heads, <span class="hljs-variable language_">self</span>.head_dim).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)    <span class="hljs-comment"># (N, heads, key_len, head_dim)</span><br>        V = V.reshape(N, value_len, <span class="hljs-variable language_">self</span>.heads, <span class="hljs-variable language_">self</span>.head_dim).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># (N, heads, value_len, head_dim)</span><br>        <br>        <span class="hljs-comment"># 3. 计算缩放点积注意力</span><br>        energy = torch.matmul(Q, K.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)) / (<span class="hljs-variable language_">self</span>.head_dim ** <span class="hljs-number">0.5</span>)  <span class="hljs-comment"># (N, heads, query_len, key_len)</span><br>        <br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            energy = energy.masked_fill(mask == <span class="hljs-number">0</span>, <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;-inf&#x27;</span>))<br>        <br>        attention = torch.softmax(energy, dim=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># (N, heads, query_len, key_len)</span><br>        attention = <span class="hljs-variable language_">self</span>.dropout(attention)<br>        <br>        <span class="hljs-comment"># 4. 加权求和得到注意力输出</span><br>        out = torch.matmul(attention, V)  <span class="hljs-comment"># (N, heads, query_len, head_dim)</span><br>        <br>        <span class="hljs-comment"># 5. 将多个头的输出拼接起来</span><br>        out = out.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).reshape(N, query_len, <span class="hljs-variable language_">self</span>.heads * <span class="hljs-variable language_">self</span>.head_dim)  <span class="hljs-comment"># (N, query_len, embed_size)</span><br>        <br>        <span class="hljs-comment"># 6. 通过输出的线性变换得到最终结果</span><br>        out = <span class="hljs-variable language_">self</span>.fc_out(out)<br>        <br>        <span class="hljs-keyword">return</span> out<br><br><span class="hljs-comment"># 测试代码</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    N = <span class="hljs-number">2</span>  <span class="hljs-comment"># batch size</span><br>    query_len = <span class="hljs-number">5</span><br>    key_len = <span class="hljs-number">5</span><br>    value_len = <span class="hljs-number">5</span><br>    embed_size = <span class="hljs-number">8</span>  <span class="hljs-comment"># embedding size</span><br>    heads = <span class="hljs-number">2</span>  <span class="hljs-comment"># number of attention heads</span><br>    <br>    <span class="hljs-comment"># 随机初始化输入</span><br>    query = torch.rand((N, query_len, embed_size))<br>    key = torch.rand((N, key_len, embed_size))<br>    value = torch.rand((N, value_len, embed_size))<br>    <br>    <span class="hljs-comment"># 初始化多头注意力模型</span><br>    attention = MultiHeadAttention(embed_size, heads)<br>    <br>    <span class="hljs-comment"># 前向传播</span><br>    output = attention(value, key, query)<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Output shape: <span class="hljs-subst">&#123;output.shape&#125;</span>&quot;</span>)<br>    <span class="hljs-comment"># Expected shape: (N, query_len, embed_size)</span><br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>精确率、召回率、准确率、AUC、ROC</title>
    <link href="/hhubbTom/hhubbTom.github.io/2025/04/04/%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E3%80%81%E5%87%86%E7%A1%AE%E7%8E%87%E3%80%81AUC%E3%80%81ROC/"/>
    <url>/hhubbTom/hhubbTom.github.io/2025/04/04/%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E3%80%81%E5%87%86%E7%A1%AE%E7%8E%87%E3%80%81AUC%E3%80%81ROC/</url>
    
    <content type="html"><![CDATA[<p>四例：第一个字母代表预测是否正确，第二个字母代表预测结果</p><p>真正例——TP，真负例——TN，假正例——FP，假负例——FN</p><h3 id="精确率、召回率、准确率"><a href="#精确率、召回率、准确率" class="headerlink" title="精确率、召回率、准确率"></a>精确率、召回率、准确率</h3><p><strong>精确率</strong>：预测为正例里面有多少实际也是正例，公式为TP&#x2F;TP+FP，实际比预测</p><p><strong>召回率</strong>：实际正例有多少被预测正确，公式为TP&#x2F;TP+FN，预测比实际</p><p><strong>准确率</strong>：所有例子被预测正确的概率，公式为TP+TN&#x2F;TP+TN+FP+FN</p><h3 id="ROC和AUC"><a href="#ROC和AUC" class="headerlink" title="ROC和AUC"></a>ROC和AUC</h3><p><strong>ROC</strong>：ROC曲线是一种用于表示分类模型性能的图形工具。它将**真阳性率（True Positive Rate，TPR）<strong>和</strong>假阳性率（False Positive Rate，FPR）**作为横纵坐标来描绘分类器在不同阈值下的性能。</p><p><img src="image.png" alt="image.png"></p><p><strong>真阳性率</strong>：即上面的召回率，代表所有实际阳性例中被检测出来的比率（1-漏诊率），越接近1越好</p><p><img src="image%201.png" alt="image.png"></p><p><strong>假阳性率</strong>：所有阴性群体中被检测出来阳性的比率(误诊率)，越接近0越好</p><p><img src="image%202.png" alt="image.png"></p><p><strong>AUC</strong>：AUC（ROC曲线下面积）是ROC曲线下的面积，用于衡量分类器性能。AUC值在0到1之间。越接近1，表示分类器性能越好；反之，AUC越接近0，表示分类器性能越差。<br>完美的分类器的AUC为1，而随机分类器的AUC为0.5。这是因为完美的分类器将所有的正例和负例完全正确地分类，而随机分类器将正例和负例的分类结果随机分布在ROC曲线上（没有分类能力）。<br><strong>为什么要用AUC？</strong></p><p>如果只用准确率accuracy，当样本类别不平衡时（如95%和5%），总是预测95%的一类，会带来误导，虽然准确率高，但实际上分类器效果并不好，AUC关注的是区分正负类的排序能力，具有更强的鲁棒性。</p><p><strong>AUC的优点</strong>：</p><p>1、不受类别不平衡影响；</p><p>2、<strong>关注排序能力</strong>，即<strong>正样本预测概率是否普遍高于负样本</strong>，不需要关注具体的决策阈值。</p><p><strong>AUC的缺点</strong>：</p><p>1、信息缺失：只能评估模型的排序能力，无法提供具体预测结果或误差的详细信息；</p><p>2、计算复杂度高：非常大的数据集计算ROC曲线会非常复杂；</p><p>3、对部分应用场景不敏感：极端情况下AUC很高，但是召回率和精度很差。</p><p><strong>计算AUC的方法</strong>：</p><p>数学意义：AUC是ROC的积分</p><p><img src="image%203.png" alt="image.png"></p><p>离散情况下AUC的计算（梯形近似）：</p><p><img src="image%204.png" alt="image.png"></p><p><strong>另一种计算方法</strong>：</p><p><img src="image%205.png" alt="image.png"></p><p><strong>依据定义计算AUC的步骤</strong>：</p><p>1、排序样本：将测试集中的样本按照模型预测的“正类概率”从高到低排序。</p><p>2、统计正负样本对：对正-负样本对，<strong>正样本预测概率高于负样本，则记为1个正确对</strong>，相等则记为0.5个正确对。</p><p>3、计算AUC：AUC的物理意义是随机选择一个正样本和一个负样本，模型对正样本预测概率高于负样本（相对顺序正确）的概率，AUC&#x3D;正确对数量&#x2F;总正负样本对数量，总正负样本对数量&#x3D;正样本数量×负样本数量。</p><p><strong>手撕AUC</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#AUC 手撕</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">AUC</span>(<span class="hljs-params">label, pre</span>):<br><span class="hljs-comment">#计算正样本和负样本的索引，以便索引出之后的概率值</span><br>    pos = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(label)) <span class="hljs-keyword">if</span> label[i] == <span class="hljs-number">1</span>]<br>    neg = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(label)) <span class="hljs-keyword">if</span> label[i] == <span class="hljs-number">0</span>]<br>    auc = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> pos:<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> neg:<br>            <span class="hljs-keyword">if</span> pre[i] &gt; pre[j]:<br>                auc += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">elif</span> pre[i] == pre[j]:<br>                auc += <span class="hljs-number">0.5</span><br>    <span class="hljs-keyword">return</span> auc / (<span class="hljs-built_in">len</span>(pos)*<span class="hljs-built_in">len</span>(neg))<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    label = [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]<br>    pre = [<span class="hljs-number">0.9</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.9</span>, <span class="hljs-number">0.66</span>, <span class="hljs-number">0.7</span>]<br>    <span class="hljs-built_in">print</span>(AUC(label, pre))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>ML、DL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
      <tag>AUC等指标</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GBDT</title>
    <link href="/hhubbTom/hhubbTom.github.io/2025/04/01/GBDT/"/>
    <url>/hhubbTom/hhubbTom.github.io/2025/04/01/GBDT/</url>
    
    <content type="html"><![CDATA[<h1 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h1><h2 id="GBDT：梯度提升决策树"><a href="#GBDT：梯度提升决策树" class="headerlink" title="GBDT：梯度提升决策树"></a><strong>GBDT：梯度提升决策树</strong></h2><p><strong>通过构造一组弱的学习器（树），并把多颗决策树的结果累加起来作为最终的预测输出</strong>。该算法将决策树与集成思想进行了有效的结合。</p><p><img src="image.png" alt="image.png"></p><h3 id="（1）boosting的思想"><a href="#（1）boosting的思想" class="headerlink" title="（1）boosting的思想"></a>（1）boosting的思想</h3><p>Boosting方法训练基分类器时采用串行的方式，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。</p><p><img src="image%201.png" alt="image 1"></p><h3 id="（2）GBDT原理"><a href="#（2）GBDT原理" class="headerlink" title="（2）GBDT原理"></a>（2）GBDT原理</h3><ul><li>所有弱分类器的结果相加等于预测值。</li><li>每次都以当前预测为基准，下一个弱分类器去拟合误差函数（上一次的误差值）对预测值的残差（预测值与真实值之间的误差）。</li><li>GBDT的弱分类器使用的是树模型（第一张图）。</li></ul><p><img src="image2.png" alt="image 2"></p><p>用GBDT去预测年龄：</p><p>第一个弱分类器（第一棵树）预测一个年龄（如20岁），计算发现误差有10岁；<br>第二棵树预测拟合残差，预测值6，计算发现差距还有4岁；<br>第三棵树继续预测拟合残差，预测值3，发现差距只有1岁了；<br>第四课树用1岁拟合剩下的残差，完成。<br>最终，四棵树的结论加起来，得到30岁这个标注答案（实际工程实现里，GBDT是计算负梯度，用负梯度近似残差）。</p><p><img src="image%203.png" alt="image.png"></p><h3 id="（3）GBDT训练过程"><a href="#（3）GBDT训练过程" class="headerlink" title="（3）GBDT训练过程"></a>（3）GBDT训练过程</h3><p>假定训练集只有4个人 (A,B,C,D)，他们的年龄分别是 (14,16,24,26)。其中，A、B分别是高一和高三学生；C、D分别是应届毕业生和工作两年的员工。</p><p>我们先看看用回归树来训练，得到的结果如下图所示：</p><p><img src="image%204.png" alt="image.png"></p><p>接下来改用GBDT来训练。由于样本数据少，我们限定叶子节点最多为2（即每棵树都只有一个分枝），并且限定树的棵树为2。最终训练得到的结果如下图所示：</p><p><img src="image%205.png" alt="image.png"></p><p>上图中的树很好理解：A、B年龄较为相近，C、D年龄较为相近，被分为左右两支，每支用平均年龄作为预测值。</p><p>我们计算残差（即「实际值」-「预测值」），所以A的残差14-15&#x3D;-1。</p><p>这里A的「预测值」是指前面所有树预测结果累加的和，在当前情形下前序只有一棵树，所以直接是15，其他多树的复杂场景下需要累加计算作为A的预测值。</p><p><img src="image%206.png" alt="image.png"></p><p>上图中的树就是残差学习的过程了</p><p>把A、B、C、D的值换作残差-1、1、-1、1，再构建一棵树学习，这棵树只有两个值1和-1，直接分成两个节点：A、C在左边，B、D在右边。</p><p>这棵树学习残差，在我们当前这个简单的场景下，已经能保证预测值和实际值（上一轮残差）相等了。</p><p>我们把这棵树的预测值累加到第一棵树上的预测结果上，就能得到真实年龄，这个简单例子中每个人都完美匹配，得到了真实的预测值。</p><p><img src="image%207.png" alt="image.png"></p><p>最终的预测过程是这样的：</p><p>A：高一学生，购物较少，经常问学长问题，真实年龄14岁，预测年龄A &#x3D; 15 – 1 &#x3D; 14<br>B：高三学生，购物较少，经常被学弟提问，真实年龄16岁，预测年龄B &#x3D; 15 + 1 &#x3D; 16<br>C：应届毕业生，购物较多，经常问学长问题，真实年龄24岁，预测年龄C &#x3D; 25 – 1 &#x3D; 24<br>D：工作两年员工，购物较多，经常被学弟提问，真实年龄26岁，预测年龄D &#x3D; 25 + 1 &#x3D; 26<br>综上，GBDT需要将多棵树的得分累加得到最终的预测得分，且每轮迭代，都是在现有树的基础上，增加一棵新的树去拟合前面树的预测值与真实值之间的残差。</p><h3 id="梯度提升-VS-梯度下降"><a href="#梯度提升-VS-梯度下降" class="headerlink" title="梯度提升 VS 梯度下降"></a>梯度提升 VS 梯度下降</h3><p>梯度提升：是在函数空间里更新</p><p>梯度下降：是在参数空间里更新</p><p><img src="image%208.png" alt="image.png"></p><h3 id="GBDT的优缺点"><a href="#GBDT的优缺点" class="headerlink" title="GBDT的优缺点"></a>GBDT的优缺点</h3><p><img src="image%209.png" alt="image.png"></p><h2 id="GBDT方法：LightGBM"><a href="#GBDT方法：LightGBM" class="headerlink" title="GBDT方法：LightGBM"></a>GBDT方法：LightGBM</h2><p>GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间</p><p>LightGBM的设计初衷就是提供一个快速高效、低内存占用、高准确度、支持并行和大规模数据处理的数据科学工具</p><p>重要特点：优化准确率，使用leaf-wise生长方式，可以处理分类变量</p><h3 id="（1）XGBoost的缺点"><a href="#（1）XGBoost的缺点" class="headerlink" title="（1）XGBoost的缺点"></a>（1）XGBoost的缺点</h3><p>XGBoost的基本思想</p><ul><li>首先，对所有特征都按照特征的数值进行<strong>预排序</strong>。</li><li>其次，在遍历分割点的时候用O(#data)的代价找到一个特征上的最好分割点。</li><li>最后，在找到一个特征的最好分割点后，将数据分裂成左右子节点。</li></ul><p><strong>总结XGBoost：预排序；Level-wise的层级生长策略；特征对梯度的访问是一种随机访问。</strong></p><p>这样的<strong>预排序算法</strong>的优点是能精确地找到分割点。但是缺点也很明显：</p><p>首先，<strong>空间消耗大</strong>。这样的算法需要保存数据的特征值信息，还保存了特征排序的结果（例如，为了后续快速的计算分割点，保存了排序后的索引），这就需要消耗训练数据两倍的内存。</p><p>其次，<strong>时间上也有较大的开销</strong>，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。</p><p>最后，<strong>对cache优化不友好</strong>。在预排序后，特征对梯度的访问是一种随机访问，<strong>并且不同的特征访问的顺序不一样</strong>，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。</p><h3 id="（2）LightGBM"><a href="#（2）LightGBM" class="headerlink" title="（2）LightGBM"></a>（2）LightGBM</h3><p><strong>LightGBM：基于Histogram的决策树算法；Leaf-wise的叶子生长策略；Cache命中率优化；直接支持类别特征（categorical Feature）</strong></p><p>LightGBM有哪些实现，各有什么区别？<br>答：gbdt:梯度提升决策树，串行速度慢，容易过拟合；rf：随机森林，并行速度快；dart：训练较慢；goss：容易过拟合。</p><p>LightGBM原理：和GBDT及XGBoost类似，都采用损失函数的负梯度作为当前决策树的残差近似值，去拟合新的决策树。</p><p>LightGBM树的生长方式是<strong>垂直方向</strong>的，其他的算法都是水平方向的，也就是说LightGBM生长的是树的叶子，其他的算法生长的是树的层次。<br>LightGBM选择具有<strong>最大误差的树叶进行生长（更改权重思想）</strong>，当生长同样的树叶，生长叶子的算法可以比基于层的算法减少更多的loss。</p><p>不建议在小数据集上使用LightGBM。LightGBM对过拟合很敏感，对于小数据集非常容易过拟合。对于过拟合的解决方法：Leaf-wise之上增加了一个最大深度的限制。</p><p>通俗解释：LGB的优化方法是，在保留大梯度（残差大）样本的同时，随机地保留一些小梯度样本，同时放大了小梯度样本带来的信息增益。</p><p>这样说起来比较抽象，我们过一遍流程： 首先把样本按照梯度排序，选出梯度最大的a%个样本，然后在剩下小梯度数据中随机选取b%个样本，在计算信息增益的时候，将选出来b%个小梯度样本的信息增益扩大（ 1 - a） &#x2F; b 倍。这样就会避免对于数据分布的改变。</p><h3 id="基于Histogram的决策树算法"><a href="#基于Histogram的决策树算法" class="headerlink" title="基于Histogram的决策树算法"></a><strong>基于Histogram的决策树算法</strong></h3><p>直方图算法的基本思想是：先把连续的浮点特征值离散化成K个整数，同时构造一个宽度为K的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。</p><p>直方图算法优点：内存占用更小，计算代价更小。</p><p>Histogram算法找到的分割点并不是很精确，但对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；较粗的分割点也有正则化的效果，可以有效地防止过拟合。</p><p>差加速：LightGBM另一个优化是Histogram（直方图）做差加速。一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到，在速度上可以提升一倍。</p><p><img src="image%2010.png" alt="image.png"></p><h3 id="单边梯度采样算法"><a href="#单边梯度采样算法" class="headerlink" title="单边梯度采样算法"></a>单边梯度采样算法</h3><p>GOSS算法从减少样本的角度出发，排除大部分小梯度的样本，仅用剩下的样本计算信息增益，即保留大梯度数据和部分小梯度数据，部分小梯度数据乘以一个系数，使得训练不足的样本得到更多关注</p><h2 id="LightGBM的优化"><a href="#LightGBM的优化" class="headerlink" title="LightGBM的优化"></a>LightGBM的优化</h2><p>（1）直接支持类别特征</p><p>（2）支持高效并行</p><p>（3）Cache命中率优化</p>]]></content>
    
    
    <categories>
      
      <category>ML、DL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>手撕梯度下降、LR、交叉熵、Softmax</title>
    <link href="/hhubbTom/hhubbTom.github.io/2025/04/01/%E6%89%8B%E6%92%95%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E3%80%81LR%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E3%80%81Softmax/"/>
    <url>/hhubbTom/hhubbTom.github.io/2025/04/01/%E6%89%8B%E6%92%95%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E3%80%81LR%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E3%80%81Softmax/</url>
    
    <content type="html"><![CDATA[<h1 id="手撕梯度下降、LR、交叉熵、Softmax"><a href="#手撕梯度下降、LR、交叉熵、Softmax" class="headerlink" title="手撕梯度下降、LR、交叉熵、Softmax"></a>手撕梯度下降、LR、交叉熵、Softmax</h1><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_loss</span>(<span class="hljs-params">w, b, x, y</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算二次损失函数</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    参数：</span><br><span class="hljs-string">    w (float): 权重</span><br><span class="hljs-string">    b (float): 偏置</span><br><span class="hljs-string">    x (float): 输入值</span><br><span class="hljs-string">    y (float): 真实标签</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    返回：</span><br><span class="hljs-string">    float: 损失值</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    y_pred = w * x + b<br>    loss = <span class="hljs-number">0.5</span> * (y - y_pred) ** <span class="hljs-number">2</span>  <span class="hljs-comment"># 二次损失</span><br>    <span class="hljs-keyword">return</span> loss<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_gradients</span>(<span class="hljs-params">w, b, x, y</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算损失函数对w和b的梯度</span><br><span class="hljs-string">    参数：</span><br><span class="hljs-string">    w (float): 权重</span><br><span class="hljs-string">    b (float): 偏置</span><br><span class="hljs-string">    x (float): 输入值</span><br><span class="hljs-string">    y (float): 真实标签</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    返回：</span><br><span class="hljs-string">    tuple: 返回梯度（dw, db）</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    y_pred = w * x + b<br>    dw = -(y - y_pred) * x  <span class="hljs-comment"># 对w的梯度</span><br>    db = -(y - y_pred)      <span class="hljs-comment"># 对b的梯度</span><br>    <span class="hljs-keyword">return</span> dw, db<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient_descent</span>(<span class="hljs-params">x, y, learning_rate=<span class="hljs-number">0.1</span>, num_iterations=<span class="hljs-number">100</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    基于梯度下降法优化w和b</span><br><span class="hljs-string">    参数：</span><br><span class="hljs-string">    x (array): 输入值数组</span><br><span class="hljs-string">    y (array): 真实标签数组</span><br><span class="hljs-string">    w_init (float): 初始权重</span><br><span class="hljs-string">    b_init (float): 初始偏置</span><br><span class="hljs-string">    learning_rate (float): 学习率</span><br><span class="hljs-string">    num_iterations (int): 迭代次数</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回：</span><br><span class="hljs-string">    tuple: 最终的优化参数w和b</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    w = <span class="hljs-number">0</span><br>    b = <span class="hljs-number">0</span><br>    loss_history = []<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iterations):<br>        dw, db = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>        total_loss = <span class="hljs-number">0</span>  <span class="hljs-comment"># 用于累加所有样本的损失值</span><br>        <span class="hljs-comment"># 计算所有样本的梯度</span><br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x)):<br>            dw_i, db_i = compute_gradients(w, b, x[j], y[j])<br>            dw += dw_i<br>            db += db_i<br>            total_loss += compute_loss(w, b, x[j], y[j])  <span class="hljs-comment"># 累加损失值</span><br><br>        <span class="hljs-comment"># 求平均梯度</span><br>        dw /= <span class="hljs-built_in">len</span>(x)<br>        db /= <span class="hljs-built_in">len</span>(x)<br><br>        <span class="hljs-comment"># 更新权重和偏置</span><br>        w -= learning_rate * dw<br>        b -= learning_rate * db<br><br>        <span class="hljs-comment"># 计算平均损失并记录</span><br>        average_loss = total_loss / <span class="hljs-built_in">len</span>(x)  <span class="hljs-comment"># 计算平均损失</span><br>        loss_history.append(average_loss)<br><br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Iteration <span class="hljs-subst">&#123;i&#125;</span>, Loss: <span class="hljs-subst">&#123;average_loss:<span class="hljs-number">.4</span>f&#125;</span>, w: <span class="hljs-subst">&#123;w:<span class="hljs-number">.4</span>f&#125;</span>, b: <span class="hljs-subst">&#123;b:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><br>    <span class="hljs-keyword">return</span> w, b, loss_history<br><br><span class="hljs-comment"># 示例数据（简单的线性回归问题）</span><br><br>x_data = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>])<br>y_data = np.array([<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>, <span class="hljs-number">11</span>])  <span class="hljs-comment"># y = 2 * x + 1</span><br><br><span class="hljs-comment"># 使用梯度下降优化w和b</span><br><br>w_final, b_final, loss_history = gradient_descent(x_data, y_data, learning_rate=<span class="hljs-number">0.1</span>, num_iterations=<span class="hljs-number">100</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;最终的权重: w = <span class="hljs-subst">&#123;w_final:<span class="hljs-number">.4</span>f&#125;</span>, 偏置: b = <span class="hljs-subst">&#123;b_final:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="LR"><a href="#LR" class="headerlink" title="LR"></a>LR</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># Sigmoid函数</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">z</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-z))<br><br><span class="hljs-comment"># 损失函数：交叉熵</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_loss</span>(<span class="hljs-params">y, y_hat</span>):<br>    m = <span class="hljs-built_in">len</span>(y)<br>    loss = -np.<span class="hljs-built_in">sum</span>(y * np.log(y_hat) + (<span class="hljs-number">1</span> - y) * np.log(<span class="hljs-number">1</span> - y_hat)) / m<br>    <span class="hljs-keyword">return</span> loss<br><br><span class="hljs-comment"># 梯度下降实现</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">logistic_regression</span>(<span class="hljs-params">X, y, learning_rate=<span class="hljs-number">0.01</span>, num_iterations=<span class="hljs-number">1000</span></span>):<br>    m, n = X.shape  <span class="hljs-comment"># m是样本数，n是特征数</span><br>    w = np.zeros(n)<br>    b = <span class="hljs-number">0</span><br>    loss_history = []<br>    <br>    <span class="hljs-comment"># 梯度下降</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_iterations):<br>        <span class="hljs-comment"># 计算线性模型输出</span><br>        z = np.dot(X, w) + b<br>        y_hat = sigmoid(z)<br>    <br>        <span class="hljs-comment"># 计算损失</span><br>        loss = compute_loss(y, y_hat)<br>        loss_history.append(loss)<br>    <br>        <span class="hljs-comment"># 计算梯度</span><br>        dw = np.dot(X.T, (y_hat - y)) / m<br>        db = np.<span class="hljs-built_in">sum</span>(y_hat - y) / m<br>    <br>        <span class="hljs-comment"># 更新参数</span><br>        w -= learning_rate * dw<br>        b -= learning_rate * db<br>    <br>        <span class="hljs-comment"># 每100次迭代输出一次损失</span><br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Iteration <span class="hljs-subst">&#123;i&#125;</span>, Loss: <span class="hljs-subst">&#123;loss:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br>    <br>    <span class="hljs-keyword">return</span> w, b, loss_history<br><br><span class="hljs-comment"># 示例数据（简单二分类问题）</span><br><br>X = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>], [<span class="hljs-number">6</span>, <span class="hljs-number">7</span>]])  <span class="hljs-comment"># 输入特征</span><br>y = np.array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])  <span class="hljs-comment"># 真实标签</span><br><br><span class="hljs-comment"># 训练逻辑回归模型</span><br><br>w, b, loss_history = logistic_regression(X, y, learning_rate=<span class="hljs-number">0.1</span>, num_iterations=<span class="hljs-number">1000</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练完成的权重:&quot;</span>, w)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练完成的偏置:&quot;</span>, b)<br></code></pre></td></tr></table></figure><h3 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 二分类交叉熵</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">binary_cross_entropy</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算二分类交叉熵损失</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    参数：</span><br><span class="hljs-string">    y_true (numpy array): 真实标签，0 或 1</span><br><span class="hljs-string">    y_pred (numpy array): 模型预测的概率（预测为正类的概率）</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    返回：</span><br><span class="hljs-string">    float: 计算得到的交叉熵损失</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 防止log(0)，对预测概率做裁剪</span><br>    epsilon = <span class="hljs-number">1e-15</span><br>    y_pred = np.clip(y_pred, epsilon, <span class="hljs-number">1.</span> - epsilon)<br>    <br>    <span class="hljs-comment"># 交叉熵公式： - [y * log(p) + (1 - y) * log(1 - p)]</span><br>    loss = - (y_true * np.log(y_pred) + (<span class="hljs-number">1</span> - y_true) * np.log(<span class="hljs-number">1</span> - y_pred))<br>    <span class="hljs-keyword">return</span> np.mean(loss)<br><br><span class="hljs-comment"># 示例：真实标签和预测概率</span><br>y_true = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>])<br>y_pred = np.array([<span class="hljs-number">0.9</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.2</span>])<br><br><span class="hljs-comment"># 计算交叉熵损失</span><br>loss = binary_cross_entropy(y_true, y_pred)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Binary Cross-Entropy Loss: <span class="hljs-subst">&#123;loss&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 多分类交叉熵</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">categorical_cross_entropy</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算多分类交叉熵损失</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    参数：</span><br><span class="hljs-string">    y_true (numpy array): 真实标签的one-hot编码（例如 [0, 1, 0]）</span><br><span class="hljs-string">    y_pred (numpy array): 模型预测的类别概率分布</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    返回：</span><br><span class="hljs-string">    float: 计算得到的交叉熵损失</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 防止log(0)，对预测概率做裁剪</span><br>    epsilon = <span class="hljs-number">1e-15</span><br>    y_pred = np.clip(y_pred, epsilon, <span class="hljs-number">1.</span> - epsilon)<br>    <br>    <span class="hljs-comment"># 交叉熵公式： - sum(p * log(q))</span><br>    loss = - np.<span class="hljs-built_in">sum</span>(y_true * np.log(y_pred), axis=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> np.mean(loss)<br><br><span class="hljs-comment"># 示例：真实标签和预测概率（one-hot编码）</span><br>y_true = np.array([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])<br>y_pred = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.1</span>], [<span class="hljs-number">0.9</span>, <span class="hljs-number">0.05</span>, <span class="hljs-number">0.05</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.8</span>], [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.1</span>], [<span class="hljs-number">0.8</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>]])<br><br><span class="hljs-comment"># 计算交叉熵损失</span><br>loss = categorical_cross_entropy(y_true, y_pred)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Categorical Cross-Entropy Loss: <span class="hljs-subst">&#123;loss&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">z</span>):<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">计算Softmax函数</span><br><span class="hljs-string">参数：</span><br><span class="hljs-string">z (numpy array): 输入向量</span><br><span class="hljs-string">返回：</span><br><span class="hljs-string">numpy array: Softmax后的概率分布</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>    exp_z = np.exp(z - np.<span class="hljs-built_in">max</span>(z))  <span class="hljs-comment"># 防止溢出，减去最大值</span><br>    <span class="hljs-keyword">return</span> exp_z / np.<span class="hljs-built_in">sum</span>(exp_z)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_derivative</span>(<span class="hljs-params">z</span>):<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">计算Softmax的导数</span><br><span class="hljs-string">参数：</span><br><span class="hljs-string">z (numpy array): 输入向量</span><br><span class="hljs-string"></span><br><span class="hljs-string">返回：</span><br><span class="hljs-string">numpy array: Softmax导数矩阵</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>    p = softmax(z)<br>    S = np.diag(p) - np.outer(p, p)  <span class="hljs-comment"># 雅可比矩阵</span><br>    <span class="hljs-keyword">return</span> S<br>    <br><span class="hljs-comment"># 示例</span><br><br>z = np.array([<span class="hljs-number">2.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.1</span>])  <span class="hljs-comment"># 模型的输出（未归一化的得分）</span><br>softmax_output = softmax(z)<br>softmax_derivative_output = softmax_derivative(z)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Softmax Output:&quot;</span>, softmax_output)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Softmax Derivative Matrix:\n&quot;</span>, softmax_derivative_output)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>ML、DL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
      <tag>梯度下降</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BN、LN</title>
    <link href="/hhubbTom/hhubbTom.github.io/2025/04/01/BN%E3%80%81LN/"/>
    <url>/hhubbTom/hhubbTom.github.io/2025/04/01/BN%E3%80%81LN/</url>
    
    <content type="html"><![CDATA[<h1 id="BN、LN"><a href="#BN、LN" class="headerlink" title="BN、LN"></a>BN、LN</h1><p><strong>一、什么是Normalization？</strong></p><p>Normalization：规范化或标准化，就是把输入数据X，在输送给神经元之前先对其进行平移和伸缩变换，将X的分布规范化成在<strong>固定区间范围</strong>的标准分布。</p><p><img src="image.png" alt="image"></p><p><strong>二、深度学习中为什么要用Normalization？</strong></p><p>把数据拉回标准<strong>正态分布</strong>，因为神经网络的Block大部分都是矩阵运算，一个向量经过矩阵运算后值会越来越大，产生各种各样的分布，梯度会不稳定。**为了网络的稳定性，加快网络的收敛，防止过拟合，且便于比较，**我们需要及时把值拉回正态分布。</p><p>Normalization根据标准化操作的维度不同可以分为batch Normalization和Layer Normalization。BatchNorm就是通过对batch size这个维度归一化来让分布稳定下来。LayerNorm则是通过对Hidden size这个维度归一化来让某层的分布稳定。</p><p><strong>三、Batch Normalization 和 Layer Normalization的定义</strong></p><p>BN（纵向规范化）：</p><p> <img src="image%201.png" alt="image 1"></p><p>LN（横向规范化）：</p><p><img src="image%202.png" alt="image 2"></p><p><strong>四、Batch Normalization 和 Layer Normalization的区别</strong></p><p>BatchNorm是对一个batch-size样本内的每个特征做归一化，**抹杀了同一样本内不同特征之间的大小关系，但是保留了不同样本间的大小关系；**LayerNorm是对每个样本的所有特征一起做归一化，<strong>抹杀了不同样本间的大小关系，但是保留了一个样本内不同特征之间的大小关系</strong>。</p><p><strong>（理解：<strong>BN对batch</strong>不同数据的同一特征</strong>进行标准化，变换之后，纵向来看，不同样本的同一特征仍然保留了之前的大小关系，但是横向对比样本内部的各个特征之间的大小关系不一定和变换之前一样了，因此抹杀或破坏了不同特征之间的大小关系，保留了不同样本之间的大小关系；LN对<strong>单一样本的所有特征</strong>进行标准化，样本内的特征处理后原来数值大的还是相对较大，原来数值小的还是相对较小，不同特征之间的大小关系还是保留了下来，但是不同样本在各自标准化处理之后，两个样本对应位置的特征之间的大小关系将不再确定，可能和处理之前就不一样了，所以破坏了不同样本间的大小关系**）**</p><p><strong>五、Batch Normalization 和 Layer Normalization的使用场景</strong></p><p>在BN和LN都能使用的场景中，BN的效果一般优于LN，原因是基于不同数据，同一特征得到的归一化特征<strong>更不容易损失信息</strong>。但是有些场景是不能使用BN的，例如batch size较小或者序列问题中使用LN。</p><p><strong>RNN 或Transformer为什么用Layer Normalization？</strong></p><p><strong>首先</strong>RNN或Transformer解决的是序列问题，一个存在的问题是不同样本的序列长度不一致，而Batch Normalization需要对不同样本的同一位置特征进行标准化处理，所以无法应用；当然，输入的序列都要做padding补齐操作，但是补齐的位置填充的都是0，这些位置都是无意义的，此时的标准化也就没有意义了。</p><p><strong>其次</strong>上面说到，BN抹杀了不同特征之间的大小关系；LN是保留了一个样本内不同特征之间的大小关系，这对NLP任务是至关重要的。对于NLP或者序列任务来说，一条样本的不同特征，其实就是时序上的变化，这正是需要学习的东西，自然不能做归一化抹杀，所以要用LN。</p>]]></content>
    
    
    <categories>
      
      <category>ML、DL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/hhubbTom/hhubbTom.github.io/2025/03/22/hello-world/"/>
    <url>/hhubbTom/hhubbTom.github.io/2025/03/22/hello-world/</url>
    
    <content type="html"><![CDATA[<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
